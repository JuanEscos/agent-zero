{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import cv2\n",
    "import copy\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import scipy\n",
    "import argparse\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import ray\n",
    "from scipy.signal import savgol_filter\n",
    "# plt.style.use('')\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "import torchvision as tv\n",
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "\n",
    "from src.common.atari_wrappers import wrap_deepmind, make_atari\n",
    "from src.common.utils import LinearSchedule, DataLoaderX, DataPrefetcher, ReplayDataset\n",
    "from src.common.vec_env import ShmemVecEnv, VecEnvWrapper, DummyVecEnv\n",
    "from src.agents.model import NatureCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "num_env = 16\n",
    "num_actors = 8\n",
    "total_steps = int(1e7)\n",
    "epoches = 1000\n",
    "update_per_data = 8\n",
    "replay_size = int(1e6)\n",
    "discount = 0.99\n",
    "batch_size = 512\n",
    "lr = 1e-3\n",
    "\n",
    "target_net_update_freq = 250\n",
    "exploration_ratio = 0.2\n",
    "steps_per_epoch = total_steps // epoches \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def make_env(game, episode_life=True, clip_rewards=True):\n",
    "    env = make_atari(f'{game}NoFrameskip-v4')\n",
    "    env = wrap_deepmind(env, episode_life=episode_life, clip_rewards=clip_rewards, frame_stack=True, scale=False, transpose_image=True)\n",
    "    return env\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--game\", type=str, default=\"Breakout\")\n",
    "    parser.add_argument(\"--replay_size\", type=int, default=int(1e6))\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1024)\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-4)\n",
    "    parser.add_argument(\"--gpu_id\", type=int, default=9)\n",
    "    parser.add_argument(\"--discount\", type=float, default=0.99)\n",
    "    parser.add_argument(\"--exploration_steps\", type=int, default=20000)\n",
    "    parser.add_argument(\"--max_step\", type=int, default=int(1e7))\n",
    "    parser.add_argument(\"--seed\", type=int, default=1234)\n",
    "    parser.add_argument(\"--num_env\", type=int, default=16)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    print(\"input args:\\n\", json.dumps(vars(args), indent=4, separators=(\",\", \":\")))\n",
    "    return args\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus=0.125)\n",
    "class Actor:\n",
    "    def __init__(self, rank, game):\n",
    "        if rank < num_actors:\n",
    "            self.envs = ShmemVecEnv([lambda: make_env(game) for _ in range(num_env)], context='fork')\n",
    "        else:\n",
    "            self.envs = ShmemVecEnv([lambda: make_env(game, True, True) for _ in range(num_env)], context='fork')\n",
    "        self.R = np.zeros(num_env)\n",
    "        self.obs = self.envs.reset()\n",
    "        self.state_shape, self.action_dim = self.envs.observation_space.shape, self.envs.action_space.n\n",
    "        self.model = NatureCNN(self.state_shape[0], self.action_dim).cuda()\n",
    "        self.rank = rank\n",
    "    \n",
    "    def sample(self, epsilon, state_dict):\n",
    "        self.model.load_state_dict(state_dict)\n",
    "        steps = steps_per_epoch // (num_env * num_actors)\n",
    "        Rs, Qs = [], []\n",
    "        tic = time.time()\n",
    "        local_replay = deque(maxlen=replay_size)\n",
    "        for step in range(steps):\n",
    "            action_random = np.random.randint(0, self.action_dim, num_env)\n",
    "            st = torch.from_numpy(np.array(self.obs)).float().cuda() / 255.0\n",
    "            qs = self.model(st)\n",
    "            qs_max, qs_argmax = qs.max(dim=-1)\n",
    "            action_greedy = qs_argmax.tolist()\n",
    "            Qs.append(qs_max.mean().item())\n",
    "            action = [act_grd if p > epsilon else act_rnd for p, act_rnd, act_grd in zip(np.random.rand(num_env), action_random, action_greedy)]\n",
    "    \n",
    "            obs_next, reward, done, info = self.envs.step(action)\n",
    "            for entry in zip(self.obs, action, reward, obs_next, done):\n",
    "                local_replay.append(entry)\n",
    "            self.obs = obs_next\n",
    "            self.R += np.array(reward)\n",
    "            for idx, d in enumerate(done):\n",
    "                if d:\n",
    "                    Rs.append(self.R[idx])\n",
    "                    self.R[idx] = 0\n",
    "        toc = time.time()\n",
    "        # print(f\"Rank {self.rank}, Data Collection Time: {toc - tic}, Speed {steps_per_epoch / (toc - tic)}\")\n",
    "        return local_replay, Rs, Qs, self.rank\n",
    "\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, game):\n",
    "        test_env = make_env(game)\n",
    "        self.state_shape, self.action_dim = test_env.observation_space.shape, test_env.action_space.n\n",
    "        self.model = NatureCNN(self.state_shape[0], self.action_dim).cuda()\n",
    "        self.model_target = copy.deepcopy(self.model).cuda()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
    "        self.replay = deque(maxlen=replay_size)\n",
    "        self.update_steps = 0\n",
    "        self.device = torch.device('cuda:0')\n",
    "        \n",
    "    def get_datafetcher(self):\n",
    "        dataset = ReplayDataset(self.replay)\n",
    "        dataloader = DataLoaderX(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        datafetcher = DataPrefetcher(dataloader, self.device)\n",
    "        return datafetcher\n",
    "    \n",
    "    def append_data(self, data):\n",
    "        self.replay.extend(data)\n",
    "    \n",
    "    def train_step(self):\n",
    "        try:\n",
    "            data = self.prefetcher.next()\n",
    "        except:\n",
    "            self.prefetcher = self.get_datafetcher()\n",
    "            data = self.prefetcher.next()\n",
    "\n",
    "        states, actions, rewards, next_states, terminals = data\n",
    "        states = states.float() / 255.0\n",
    "        next_states = next_states.float() / 255.0\n",
    "        actions = actions.long()\n",
    "        terminals = terminals.float()\n",
    "        rewards = rewards.float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_next = self.model_target(next_states)\n",
    "            q_next_online = self.model(next_states)\n",
    "            q_next = q_next.gather(1, q_next_online.argmax(dim=-1).unsqueeze(-1)).squeeze(-1)\n",
    "            q_target = rewards + discount * (1 - terminals) * q_next\n",
    "\n",
    "        q = self.model(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "        loss = F.smooth_l1_loss(q, q_target)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.update_steps += 1\n",
    "\n",
    "        if self.update_steps % 250 == 0:\n",
    "            self.model_target.load_state_dict(self.model.state_dict())\n",
    "        return loss.detach()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def formated_print(var_name, xs):\n",
    "    print(\"{0}\\t {1:12.6f}\\t{2:12.6f}\\t{3:12.6f}\\t\".format(var_name, np.mean(xs), np.std(xs), np.max(xs)))\n",
    "\n",
    "def train(game):\n",
    "    ray.init()\n",
    "    \n",
    "    epsilon_schedule = LinearSchedule(1.0, 0.01, int(total_steps * exploration_ratio))\n",
    "    actors = [Actor.remote(rank, game) for rank in range(num_actors)]\n",
    "    tester = Actor.remote(num_actors, game)\n",
    "    \n",
    "    agent = Agent(game)\n",
    "    sample_ops = [a.sample.remote(1.0, agent.model.state_dict()) for a in actors]\n",
    "    test_op = tester.sample.remote(0.01, agent.model.state_dict())\n",
    "    \n",
    "    TRRs, RRs, QQs, LLs = [], [], [], []\n",
    "    for local_replay, Rs, Qs, rank in ray.get(sample_ops + [test_op]):\n",
    "        if rank < num_actors:\n",
    "            agent.append_data(local_replay)\n",
    "            RRs += Rs\n",
    "            QQs += Qs\n",
    "        else:\n",
    "            TRRs += Rs\n",
    "            \n",
    "    \n",
    "    print(\"Warming up reward:\", np.mean(RRs), np.std(RRs), np.max(RRs))\n",
    "    print(\"Warming up Qmax:\", np.mean(QQs), np.std(QQs), np.max(QQs))        \n",
    "        \n",
    "    steps = 0\n",
    "    epoch = 0\n",
    "    tic = time.time()\n",
    "    while True:\n",
    "        done_id, sample_ops = ray.wait(sample_ops)\n",
    "        data = ray.get(done_id)\n",
    "        local_replay, Rs, Qs, rank = data[0]\n",
    "        \n",
    "        if rank < num_actors:\n",
    "            # Actor\n",
    "            agent.append_data(local_replay)\n",
    "            steps += len(local_replay)\n",
    "            epsilon = epsilon_schedule(len(local_replay))\n",
    "            sample_ops.append(actors[rank].sample.remote(epsilon, agent.model.state_dict()))\n",
    "            \n",
    "            RRs += Rs\n",
    "            QQs += Qs\n",
    "        else:\n",
    "            # Tester\n",
    "            sample_ops.append(tester.sample.remote(0.01, agent.model.state_dict()))\n",
    "            TRRs += Rs\n",
    "        \n",
    "        # Trainer\n",
    "        for _ in range(20):\n",
    "            loss = agent.train_step()\n",
    "            LLs.append(loss)\n",
    "        \n",
    "        if (steps // steps_per_epoch) > epoch:\n",
    "            if epoch % 10 == 0:\n",
    "                toc = time.time()\n",
    "                print(\"=\" * 100)\n",
    "                print(f\"Epoch: {epoch:5d}\\t Steps: {steps:10d}\\t Average Speed: {steps / (toc - tic):8.2f}\\t Epsilon: {epsilon}\")\n",
    "                print('-' * 100)\n",
    "                formated_print(\"EP Training Reward\", RRs[-1000:])\n",
    "                formated_print(\"EP Loss           \", torch.stack(LLs).tolist()[-1000:])\n",
    "                formated_print(\"EP Qmax           \", QQs[-1000:])\n",
    "                if len(TRRs) > 0:\n",
    "                    formated_print(\"EP Test Reward    \", TRRs[-1000:])\n",
    "                    \n",
    "                print(\"=\" * 100)\n",
    "                print(\" \" * 100)\n",
    "                \n",
    "                torch.save({\n",
    "                    'model': agent.model.state_dict(),\n",
    "                    'optim': agent.optimizer.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'epsilon': epsilon,\n",
    "                    'steps': steps,\n",
    "                    'Rs': RRs,\n",
    "                    'TRs': TRRs,\n",
    "                    'Qs': QQs,\n",
    "                    'Ls': LLs,\n",
    "                }, f'ckpt/{game}_e{epoch}')\n",
    "            \n",
    "            epoch += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "2020-08-03 06:53:13,089\tINFO resource_spec.py:212 -- Starting Ray with 231.15 GiB memory available for workers and up to 103.06 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-08-03 06:53:13,359\tWARNING services.py:923 -- Redis failed to start, retrying now.\n",
      "2020-08-03 06:53:13,749\tINFO services.py:1165 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n",
      "/home/bzhou/miniconda3/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "\u001b[2m\u001b[36m(pid=44672)\u001b[0m Logging to /tmp/openai-2020-08-03-06-53-15-573359\n",
      "\u001b[2m\u001b[36m(pid=44672)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=44670)\u001b[0m Logging to /tmp/openai-2020-08-03-06-53-15-587410\n",
      "\u001b[2m\u001b[36m(pid=44670)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=44667)\u001b[0m Logging to /tmp/openai-2020-08-03-06-53-15-585990\n",
      "\u001b[2m\u001b[36m(pid=44667)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=44674)\u001b[0m Logging to /tmp/openai-2020-08-03-06-53-15-592306\n",
      "\u001b[2m\u001b[36m(pid=44674)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=44668)\u001b[0m Logging to /tmp/openai-2020-08-03-06-53-15-592589\n",
      "\u001b[2m\u001b[36m(pid=44668)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=44682)\u001b[0m Logging to /tmp/openai-2020-08-03-06-53-15-595843\n",
      "\u001b[2m\u001b[36m(pid=44682)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=44687)\u001b[0m Logging to /tmp/openai-2020-08-03-06-53-15-618094\n",
      "\u001b[2m\u001b[36m(pid=44687)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=44705)\u001b[0m Logging to /tmp/openai-2020-08-03-06-53-15-627876\n",
      "\u001b[2m\u001b[36m(pid=44705)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=44718)\u001b[0m Logging to /tmp/openai-2020-08-03-06-53-15-689326\n",
      "\u001b[2m\u001b[36m(pid=44718)\u001b[0m Creating dummy env object to get spaces\n",
      "Warming up reward: 0.04262295081967213 0.20200553181310638 1.0\n",
      "Warming up Qmax: -0.16281540264399388 0.004737366926235457 -0.14930219948291779\n",
      "====================================================================================================\n",
      "Epoch:     0\t Steps:      11232\t Average Speed:  1413.96\t Epsilon: 0.9950579199999998\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     0.068038\t    0.281481\t    2.000000\t\n",
      "EP Loss           \t     0.015358\t    0.152251\t    2.040676\t\n",
      "EP Qmax           \t    -0.161001\t    0.006980\t   -0.140314\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    10\t Steps:     111072\t Average Speed:  2071.27\t Epsilon: 0.945637119999998\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     0.297000\t    0.593962\t    4.000000\t\n",
      "EP Loss           \t     0.000788\t    0.000600\t    0.004437\t\n",
      "EP Qmax           \t    -0.049295\t    0.051084\t    0.142516\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    20\t Steps:     210912\t Average Speed:  2212.15\t Epsilon: 0.8962163199999962\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     0.255000\t    0.569188\t    3.000000\t\n",
      "EP Loss           \t     0.000590\t    0.000434\t    0.003291\t\n",
      "EP Qmax           \t     0.024518\t    0.069329\t    0.258400\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    30\t Steps:     310752\t Average Speed:  2264.76\t Epsilon: 0.8467955199999944\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     0.264000\t    0.562409\t    3.000000\t\n",
      "EP Loss           \t     0.000840\t    0.000494\t    0.003912\t\n",
      "EP Qmax           \t     0.124651\t    0.076800\t    0.356616\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    40\t Steps:     410592\t Average Speed:  2300.89\t Epsilon: 0.7973747199999925\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     0.392000\t    0.685810\t    4.000000\t\n",
      "EP Loss           \t     0.001610\t    0.000753\t    0.005835\t\n",
      "EP Qmax           \t     0.305158\t    0.087305\t    0.591129\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    50\t Steps:     510432\t Average Speed:  2326.55\t Epsilon: 0.7479539199999907\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     0.608000\t    0.929697\t    7.000000\t\n",
      "EP Loss           \t     0.001817\t    0.000824\t    0.008318\t\n",
      "EP Qmax           \t     0.513905\t    0.092152\t    0.763121\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    60\t Steps:     610272\t Average Speed:  2323.14\t Epsilon: 0.6985331199999889\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     0.600000\t    0.988939\t    5.000000\t\n",
      "EP Loss           \t     0.001503\t    0.000500\t    0.003998\t\n",
      "EP Qmax           \t     0.636860\t    0.092707\t    0.899174\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # args = parse_arguments()\n",
    "    # game = args.game\n",
    "    game = \"Breakout\"\n",
    "    train(game)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plt.plot(TRs)\n",
    "# device = torch.device('cuda:0')\n",
    "# replay = deque(maxlen=10000)\n",
    "# datas = [np.random.randn(1000, 32)] * 5\n",
    "# for entry in zip(*datas):\n",
    "#     replay.append(entry)\n",
    "#     \n",
    "# dataset = ReplayDataset(replay)\n",
    "# dataloader = DataLoaderX(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "# print(next(iter(dataloader)))\n",
    "# datafetcher = DataPrefetcher(dataloader, device)\n",
    "# datafetcher.preload()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# LLs = []\n",
    "# RRs = []\n",
    "# for epoch in range(10):\n",
    "#     model, model_target, Ls = train(model, model_target)\n",
    "#     # Rs = test(model)\n",
    "#     LLs += Ls\n",
    "#     # RRs += Rs\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plt.plot(Ls)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('replay.pkl', 'wb') as f:\n",
    "#     pickle.dump(replay, f, pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}