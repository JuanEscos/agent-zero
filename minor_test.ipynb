{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import cv2\n",
    "import copy\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import scipy\n",
    "import argparse\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import ray\n",
    "from scipy.signal import savgol_filter\n",
    "# plt.style.use('')\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "import torchvision as tv\n",
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "\n",
    "from src.common.atari_wrappers import wrap_deepmind, make_atari\n",
    "from src.common.utils import LinearSchedule, DataLoaderX, DataPrefetcher, ReplayDataset\n",
    "from src.common.vec_env import ShmemVecEnv, VecEnvWrapper, DummyVecEnv\n",
    "from src.agents.model import NatureCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "num_env = 16\n",
    "num_actors = 8\n",
    "total_steps = int(1e7)\n",
    "epoches = 1000\n",
    "update_per_data = 8\n",
    "replay_size = int(1e6)\n",
    "discount = 0.99\n",
    "batch_size = 512\n",
    "lr = 1e-3\n",
    "\n",
    "target_net_update_freq = 250\n",
    "exploration_ratio = 0.2\n",
    "steps_per_epoch = total_steps // epoches \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def make_env(game, episode_life=True, clip_rewards=True):\n",
    "    env = make_atari(f'{game}NoFrameskip-v4')\n",
    "    env = wrap_deepmind(env, episode_life=episode_life, clip_rewards=clip_rewards, frame_stack=True, scale=False, transpose_image=True)\n",
    "    return env\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--game\", type=str, default=\"Breakout\")\n",
    "    parser.add_argument(\"--replay_size\", type=int, default=int(1e6))\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1024)\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-4)\n",
    "    parser.add_argument(\"--gpu_id\", type=int, default=9)\n",
    "    parser.add_argument(\"--discount\", type=float, default=0.99)\n",
    "    parser.add_argument(\"--exploration_steps\", type=int, default=20000)\n",
    "    parser.add_argument(\"--max_step\", type=int, default=int(1e7))\n",
    "    parser.add_argument(\"--seed\", type=int, default=1234)\n",
    "    parser.add_argument(\"--num_env\", type=int, default=16)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    print(\"input args:\\n\", json.dumps(vars(args), indent=4, separators=(\",\", \":\")))\n",
    "    return args\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus=0.125)\n",
    "class Actor:\n",
    "    def __init__(self, rank, game):\n",
    "        if rank < num_actors:\n",
    "            self.envs = ShmemVecEnv([lambda: make_env(game) for _ in range(num_env)], context='fork')\n",
    "        else:\n",
    "            self.envs = ShmemVecEnv([lambda: make_env(game, False, False) for _ in range(num_env)], context='fork')\n",
    "        self.R = np.zeros(num_env)\n",
    "        self.obs = self.envs.reset()\n",
    "        self.state_shape, self.action_dim = self.envs.observation_space.shape, self.envs.action_space.n\n",
    "        self.model = NatureCNN(self.state_shape[0], self.action_dim).cuda()\n",
    "        self.rank = rank\n",
    "    \n",
    "    def sample(self, epsilon, state_dict):\n",
    "        self.model.load_state_dict(state_dict)\n",
    "        steps = steps_per_epoch // (num_env * num_actors)\n",
    "        Rs, Qs = [], []\n",
    "        tic = time.time()\n",
    "        local_replay = deque(maxlen=replay_size)\n",
    "        for step in range(steps):\n",
    "            action_random = np.random.randint(0, self.action_dim, num_env)\n",
    "            st = torch.from_numpy(np.array(self.obs)).float().cuda() / 255.0\n",
    "            qs = self.model(st)\n",
    "            qs_max, qs_argmax = qs.max(dim=-1)\n",
    "            action_greedy = qs_argmax.tolist()\n",
    "            Qs.append(qs_max.mean().item())\n",
    "            action = [act_grd if p > epsilon else act_rnd for p, act_rnd, act_grd in zip(np.random.rand(num_env), action_random, action_greedy)]\n",
    "    \n",
    "            obs_next, reward, done, info = self.envs.step(action)\n",
    "            for entry in zip(self.obs, action, reward, obs_next, done):\n",
    "                local_replay.append(entry)\n",
    "            self.obs = obs_next\n",
    "            self.R += np.array(reward)\n",
    "            for idx, d in enumerate(done):\n",
    "                if d:\n",
    "                    Rs.append(self.R[idx])\n",
    "                    self.R[idx] = 0\n",
    "        toc = time.time()\n",
    "        # print(f\"Rank {self.rank}, Data Collection Time: {toc - tic}, Speed {steps_per_epoch / (toc - tic)}\")\n",
    "        return local_replay, Rs, Qs, self.rank\n",
    "\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, game):\n",
    "        test_env = make_env(game)\n",
    "        self.state_shape, self.action_dim = test_env.observation_space.shape, test_env.action_space.n\n",
    "        self.model = NatureCNN(self.state_shape[0], self.action_dim).cuda()\n",
    "        self.model_target = copy.deepcopy(self.model).cuda()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
    "        self.replay = deque(maxlen=replay_size)\n",
    "        self.update_steps = 0\n",
    "        self.device = torch.device('cuda:0')\n",
    "        \n",
    "    def get_datafetcher(self):\n",
    "        dataset = ReplayDataset(self.replay)\n",
    "        dataloader = DataLoaderX(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        datafetcher = DataPrefetcher(dataloader, self.device)\n",
    "        return datafetcher\n",
    "    \n",
    "    def append_data(self, data):\n",
    "        self.replay.extend(data)\n",
    "    \n",
    "    def train_step(self):\n",
    "        try:\n",
    "            data = self.prefetcher.next()\n",
    "        except:\n",
    "            self.prefetcher = self.get_datafetcher()\n",
    "            data = self.prefetcher.next()\n",
    "\n",
    "        states, actions, rewards, next_states, terminals = data\n",
    "        states = states.float() / 255.0\n",
    "        next_states = next_states.float() / 255.0\n",
    "        actions = actions.long()\n",
    "        terminals = terminals.float()\n",
    "        rewards = rewards.float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_next = self.model_target(next_states)\n",
    "            q_next_online = self.model(next_states)\n",
    "            q_next = q_next.gather(1, q_next_online.argmax(dim=-1).unsqueeze(-1)).squeeze(-1)\n",
    "            q_target = rewards + discount * (1 - terminals) * q_next\n",
    "\n",
    "        q = self.model(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "        loss = F.smooth_l1_loss(q, q_target)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.update_steps += 1\n",
    "\n",
    "        if self.update_steps % 250 == 0:\n",
    "            self.model_target.load_state_dict(self.model.state_dict())\n",
    "        return loss.detach()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def formated_print(var_name, xs):\n",
    "    if len(xs) > 0:\n",
    "        print(\"{0} mean/std/max/min\\t {1:12.6f}\\t{2:12.6f}\\t{3:12.6f}\\t{4:12.6}\".format(\n",
    "            var_name, np.mean(xs), np.std(xs), np.max(xs), np.min(xs)))\n",
    "\n",
    "def train(game):\n",
    "    ray.init()\n",
    "    epsilon_schedule = LinearSchedule(1.0, 0.01, int(total_steps * exploration_ratio))\n",
    "    actors = [Actor.remote(rank, game) for rank in range(num_actors + 1)]\n",
    "    tester = actors[-1]\n",
    "    \n",
    "    agent = Agent(game)\n",
    "    sample_ops = [a.sample.remote(1.0, agent.model.state_dict()) for a in actors]\n",
    "    \n",
    "    TRRs, RRs, QQs, LLs = [], [], [], []\n",
    "    for local_replay, Rs, Qs, rank in ray.get(sample_ops): # + [test_op]):\n",
    "        if rank < num_actors:\n",
    "            agent.append_data(local_replay)\n",
    "            RRs += Rs\n",
    "            QQs += Qs\n",
    "        else:\n",
    "            TRRs += Rs\n",
    "            \n",
    "    \n",
    "    formated_print(\"Warming up Reward\", RRs)\n",
    "    formated_print(\"Warming up Qmax\", QQs)\n",
    "        \n",
    "    steps = 0\n",
    "    epoch = 0\n",
    "    tic = time.time()\n",
    "    while True:\n",
    "        done_id, sample_ops = ray.wait(sample_ops)\n",
    "        data = ray.get(done_id)\n",
    "        local_replay, Rs, Qs, rank = data[0]\n",
    "        \n",
    "        if rank < num_actors:\n",
    "            # Actor\n",
    "            agent.append_data(local_replay)\n",
    "            steps += len(local_replay)\n",
    "            epsilon = epsilon_schedule(len(local_replay))\n",
    "            if epsilon == 0.01:\n",
    "                epsilon=np.random.choice([0.01, 0.02, 0.05, 0.1], p=[0.7, 0.1, 0.1, 0.1])\n",
    "            \n",
    "            sample_ops.append(actors[rank].sample.remote(epsilon, agent.model.state_dict()))\n",
    "            RRs += Rs\n",
    "            QQs += Qs\n",
    "        else:\n",
    "            # Tester\n",
    "            sample_ops.append(tester.sample.remote(0.01, agent.model.state_dict()))\n",
    "            TRRs += Rs\n",
    "        \n",
    "        # Trainer\n",
    "        for _ in range(16):\n",
    "            loss = agent.train_step()\n",
    "            LLs.append(loss)\n",
    "        \n",
    "        if (steps // steps_per_epoch) > epoch:\n",
    "            if epoch % 10 == 0:\n",
    "                toc = time.time()\n",
    "                print(\"=\" * 100)\n",
    "                print(f\"Epoch: {epoch:5d}\\t Steps: {steps:10d}\\t Average Speed: {steps / (toc - tic):8.2f}\\t Epsilon: {epsilon}\")\n",
    "                print('-' * 100)\n",
    "                formated_print(\"EP Training Reward\", RRs[-1000:])\n",
    "                formated_print(\"EP Loss           \", torch.stack(LLs).tolist()[-1000:])\n",
    "                formated_print(\"EP Qmax           \", QQs[-1000:])\n",
    "                if len(TRRs) > 0:\n",
    "                    formated_print(\"EP Test Reward    \", TRRs[-1000:])\n",
    "                    \n",
    "                print(\"=\" * 100)\n",
    "                print(\" \" * 100)\n",
    "            \n",
    "            if epoch % 50 == 0:\n",
    "                torch.save({\n",
    "                    'model': agent.model.state_dict(),\n",
    "                    'optim': agent.optimizer.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'epsilon': epsilon,\n",
    "                    'steps': steps,\n",
    "                    'Rs': RRs,\n",
    "                    'TRs': TRRs,\n",
    "                    'Qs': QQs,\n",
    "                    'Ls': LLs,\n",
    "                    'time': toc - tic,\n",
    "                }, f'ckpt/{game}_e{epoch:04d}.pth')\n",
    "            \n",
    "            epoch += 1\n",
    "            if epoch == 10:\n",
    "                sample_ops.append(tester.sample.remote(0.01, agent.model.state_dict()))\n",
    "            if epoch > epoches:\n",
    "                ray.shutdown()\n",
    "                return\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "2020-08-03 14:25:25,577\tINFO resource_spec.py:212 -- Starting Ray with 202.49 GiB memory available for workers and up to 90.79 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-08-03 14:25:25,695\tWARNING services.py:923 -- Redis failed to start, retrying now.\n",
      "2020-08-03 14:25:25,932\tWARNING services.py:923 -- Redis failed to start, retrying now.\n",
      "2020-08-03 14:25:26,259\tINFO services.py:1165 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8266\u001b[39m\u001b[22m\n",
      "/home/bzhou/miniconda3/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "\u001b[2m\u001b[36m(pid=33813)\u001b[0m Logging to /tmp/openai-2020-08-03-14-25-28-292625\n",
      "\u001b[2m\u001b[36m(pid=33813)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=33816)\u001b[0m Logging to /tmp/openai-2020-08-03-14-25-28-284496\n",
      "\u001b[2m\u001b[36m(pid=33816)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=33832)\u001b[0m Logging to /tmp/openai-2020-08-03-14-25-28-335593\n",
      "\u001b[2m\u001b[36m(pid=33832)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=33806)\u001b[0m Logging to /tmp/openai-2020-08-03-14-25-28-314977\n",
      "\u001b[2m\u001b[36m(pid=33806)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=33789)\u001b[0m Logging to /tmp/openai-2020-08-03-14-25-28-324392\n",
      "\u001b[2m\u001b[36m(pid=33789)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=33821)\u001b[0m Logging to /tmp/openai-2020-08-03-14-25-28-324411\n",
      "\u001b[2m\u001b[36m(pid=33821)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=33811)\u001b[0m Logging to /tmp/openai-2020-08-03-14-25-28-347965\n",
      "\u001b[2m\u001b[36m(pid=33811)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=33791)\u001b[0m Logging to /tmp/openai-2020-08-03-14-25-28-380515\n",
      "\u001b[2m\u001b[36m(pid=33791)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=33842)\u001b[0m Logging to /tmp/openai-2020-08-03-14-25-28-376455\n",
      "\u001b[2m\u001b[36m(pid=33842)\u001b[0m Creating dummy env object to get spaces\n",
      "Warming up Reward mean/std/max/min\t     0.062500\t    0.242061\t    1.000000\t         0.0\n",
      "Warming up Qmax mean/std/max/min\t    -0.281937\t    0.004910\t   -0.270867\t   -0.294874\n",
      "====================================================================================================\n",
      "Epoch:     0\t Steps:      11232\t Average Speed:  1693.97\t Epsilon: 0.9950579199999998\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward mean/std/max/min\t     0.096552\t    0.338845\t    2.000000\t         0.0\n",
      "EP Loss            mean/std/max/min\t     0.025911\t    0.219958\t    2.617150\t 0.000731049\n",
      "EP Qmax            mean/std/max/min\t    -0.278686\t    0.009667\t   -0.253378\t   -0.294874\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    10\t Steps:     111072\t Average Speed:  2070.93\t Epsilon: 0.945637119999998\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward mean/std/max/min\t     0.285000\t    0.606445\t    4.000000\t         0.0\n",
      "EP Loss            mean/std/max/min\t     0.001060\t    0.000675\t    0.005569\t 0.000259729\n",
      "EP Qmax            mean/std/max/min\t    -0.163522\t    0.050955\t    0.021761\t   -0.257279\n",
      "EP Test Reward     mean/std/max/min\t     1.772727\t    1.443853\t    6.000000\t         0.0\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    20\t Steps:     210912\t Average Speed:  2137.23\t Epsilon: 0.8962163199999962\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward mean/std/max/min\t     0.257000\t    0.587325\t    4.000000\t         0.0\n",
      "EP Loss            mean/std/max/min\t     0.000758\t    0.000435\t    0.003775\t  0.00023088\n",
      "EP Qmax            mean/std/max/min\t    -0.038569\t    0.065264\t    0.161064\t   -0.151915\n",
      "EP Test Reward     mean/std/max/min\t     1.652482\t    1.419149\t    6.000000\t         0.0\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    30\t Steps:     310752\t Average Speed:  2217.11\t Epsilon: 0.8467955199999944\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward mean/std/max/min\t     0.258000\t    0.602857\t    4.000000\t         0.0\n",
      "EP Loss            mean/std/max/min\t     0.000888\t    0.000423\t    0.003166\t 0.000277896\n",
      "EP Qmax            mean/std/max/min\t     0.063942\t    0.070506\t    0.285032\t   -0.085662\n",
      "EP Test Reward     mean/std/max/min\t     1.390110\t    1.401142\t    6.000000\t         0.0\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    40\t Steps:     410592\t Average Speed:  2257.23\t Epsilon: 0.7973747199999925\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward mean/std/max/min\t     0.327000\t    0.635666\t    4.000000\t         0.0\n",
      "EP Loss            mean/std/max/min\t     0.000870\t    0.000350\t    0.002527\t 0.000306299\n",
      "EP Qmax            mean/std/max/min\t     0.183807\t    0.089278\t    0.480292\t -0.00263432\n",
      "EP Test Reward     mean/std/max/min\t     1.611321\t    1.490810\t    7.000000\t         0.0\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    50\t Steps:     510432\t Average Speed:  2277.40\t Epsilon: 0.7479539199999907\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward mean/std/max/min\t     0.610000\t    0.876299\t    5.000000\t         0.0\n",
      "EP Loss            mean/std/max/min\t     0.001173\t    0.000456\t    0.004098\t  0.00044174\n",
      "EP Qmax            mean/std/max/min\t     0.382772\t    0.090254\t    0.726644\t    0.135992\n",
      "EP Test Reward     mean/std/max/min\t     2.347973\t    2.797899\t   16.000000\t         0.0\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    60\t Steps:     610272\t Average Speed:  2326.16\t Epsilon: 0.6985331199999889\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward mean/std/max/min\t     0.589000\t    0.902263\t    4.000000\t         0.0\n",
      "EP Loss            mean/std/max/min\t     0.001325\t    0.000452\t    0.004125\t 0.000603914\n",
      "EP Qmax            mean/std/max/min\t     0.521430\t    0.091890\t    0.845149\t    0.254966\n",
      "EP Test Reward     mean/std/max/min\t     3.003185\t    4.026582\t   24.000000\t         0.0\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    70\t Steps:     710112\t Average Speed:  2296.78\t Epsilon: 0.6491123199999871\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward mean/std/max/min\t     0.698000\t    1.062448\t    6.000000\t         0.0\n",
      "EP Loss            mean/std/max/min\t     0.001276\t    0.000409\t    0.004061\t 0.000616321\n",
      "EP Qmax            mean/std/max/min\t     0.607545\t    0.084420\t    0.869179\t    0.382004\n",
      "EP Test Reward     mean/std/max/min\t     4.578797\t    6.305745\t   31.000000\t         0.0\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    80\t Steps:     811200\t Average Speed:  2301.24\t Epsilon: 0.5990737599999852\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward mean/std/max/min\t     0.846000\t    1.195945\t    7.000000\t         0.0\n",
      "EP Loss            mean/std/max/min\t     0.001268\t    0.000370\t    0.002781\t 0.000556138\n",
      "EP Qmax            mean/std/max/min\t     0.716306\t    0.089806\t    0.988525\t    0.432135\n",
      "EP Test Reward     mean/std/max/min\t     5.654891\t    7.788504\t   38.000000\t         0.0\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    90\t Steps:     911040\t Average Speed:  2316.68\t Epsilon: 0.5496529599999834\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward mean/std/max/min\t     0.925000\t    1.357709\t   10.000000\t         0.0\n",
      "EP Loss            mean/std/max/min\t     0.001239\t    0.000388\t    0.002907\t 0.000545586\n",
      "EP Qmax            mean/std/max/min\t     0.784120\t    0.099589\t    1.124607\t    0.475785\n",
      "EP Test Reward     mean/std/max/min\t     6.872727\t    9.674543\t   53.000000\t         0.0\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   100\t Steps:    1010880\t Average Speed:  2311.09\t Epsilon: 0.5002321599999816\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward mean/std/max/min\t     1.223000\t    1.554758\t    9.000000\t         0.0\n",
      "EP Loss            mean/std/max/min\t     0.002006\t    0.002590\t    0.044776\t 0.000596276\n",
      "EP Qmax            mean/std/max/min\t     0.938299\t    0.110640\t    1.235955\t    0.607974\n",
      "EP Test Reward     mean/std/max/min\t     7.570707\t   10.552541\t   53.000000\t         0.0\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   110\t Steps:    1110720\t Average Speed:  2293.45\t Epsilon: 0.4508113599999798\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward mean/std/max/min\t     1.456000\t    1.721065\t   12.000000\t         0.0\n",
      "EP Loss            mean/std/max/min\t     0.003506\t    0.001297\t    0.009463\t  0.00124336\n",
      "EP Qmax            mean/std/max/min\t     1.026179\t    0.107872\t    1.354223\t    0.633767\n",
      "EP Test Reward     mean/std/max/min\t     8.954436\t   12.092979\t   53.000000\t         0.0\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   120\t Steps:    1210560\t Average Speed:  2281.59\t Epsilon: 0.40139055999997797\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward mean/std/max/min\t     1.764000\t    2.102452\t   16.000000\t         0.0\n",
      "EP Loss            mean/std/max/min\t     0.002933\t    0.001119\t    0.008500\t  0.00104259\n",
      "EP Qmax            mean/std/max/min\t     1.121557\t    0.107102\t    1.369192\t    0.808492\n",
      "EP Test Reward     mean/std/max/min\t    10.171296\t   13.629295\t   63.000000\t         0.0\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   130\t Steps:    1310400\t Average Speed:  2261.31\t Epsilon: 0.35196975999997615\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward mean/std/max/min\t     2.031000\t    2.236971\t   14.000000\t         0.0\n",
      "EP Loss            mean/std/max/min\t     0.003004\t    0.001029\t    0.006921\t 0.000897995\n",
      "EP Qmax            mean/std/max/min\t     1.190575\t    0.106143\t    1.494807\t    0.819707\n",
      "EP Test Reward     mean/std/max/min\t    12.096197\t   18.944217\t  233.000000\t         0.0\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   140\t Steps:    1410240\t Average Speed:  2249.26\t Epsilon: 0.30254895999997433\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward mean/std/max/min\t     2.478000\t    2.597983\t   16.000000\t         0.0\n",
      "EP Loss            mean/std/max/min\t     0.003092\t    0.001088\t    0.008198\t  0.00113258\n",
      "EP Qmax            mean/std/max/min\t     1.325025\t    0.104205\t    1.586163\t    0.899867\n",
      "EP Test Reward     mean/std/max/min\t    13.795259\t   20.629404\t  233.000000\t         0.0\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   150\t Steps:    1510080\t Average Speed:  2233.01\t Epsilon: 0.2531281599999725\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward mean/std/max/min\t     3.047000\t    2.915955\t   18.000000\t         0.0\n",
      "EP Loss            mean/std/max/min\t     0.003706\t    0.001295\t    0.008630\t   0.0013654\n",
      "EP Qmax            mean/std/max/min\t     1.429092\t    0.096147\t    1.663074\t     1.05831\n",
      "EP Test Reward     mean/std/max/min\t    15.390397\t   22.372160\t  233.000000\t         0.0\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   160\t Steps:    1611168\t Average Speed:  2204.53\t Epsilon: 0.20308959999997278\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward mean/std/max/min\t     3.360000\t    3.004064\t   20.000000\t         0.0\n",
      "EP Loss            mean/std/max/min\t     0.003891\t    0.001364\t    0.009975\t  0.00133456\n",
      "EP Qmax            mean/std/max/min\t     1.514141\t    0.103002\t    1.811957\t     1.18298\n",
      "EP Test Reward     mean/std/max/min\t    17.354000\t   23.950797\t  233.000000\t         0.0\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   170\t Steps:    1711008\t Average Speed:  2195.20\t Epsilon: 0.15366879999997318\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward mean/std/max/min\t     4.098000\t    3.682716\t   21.000000\t         0.0\n",
      "EP Loss            mean/std/max/min\t     0.004935\t    0.001547\t    0.011701\t  0.00177584\n",
      "EP Qmax            mean/std/max/min\t     1.608299\t    0.110777\t    1.967474\t     1.21275\n",
      "EP Test Reward     mean/std/max/min\t    19.077369\t   25.467881\t  233.000000\t         0.0\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   180\t Steps:    1810848\t Average Speed:  2189.98\t Epsilon: 0.10424799999997358\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward mean/std/max/min\t     5.126000\t    4.760475\t   66.000000\t         0.0\n",
      "EP Loss            mean/std/max/min\t     0.004799\t    0.001597\t    0.013202\t  0.00157148\n",
      "EP Qmax            mean/std/max/min\t     1.709737\t    0.086693\t    1.955895\t     1.36913\n",
      "EP Test Reward     mean/std/max/min\t    20.614232\t   26.546494\t  233.000000\t         0.0\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   190\t Steps:    1910688\t Average Speed:  2164.88\t Epsilon: 0.054827199999973895\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward mean/std/max/min\t     6.091000\t    5.465228\t   66.000000\t         0.0\n",
      "EP Loss            mean/std/max/min\t     0.004672\t    0.001446\t    0.009657\t   0.0018403\n",
      "EP Qmax            mean/std/max/min\t     1.786680\t    0.086062\t    2.020674\t      1.5302\n",
      "EP Test Reward     mean/std/max/min\t    22.632727\t   30.221119\t  305.000000\t         0.0\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   200\t Steps:    2010528\t Average Speed:  2160.52\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward mean/std/max/min\t     6.985000\t    6.040097\t   37.000000\t         0.0\n",
      "EP Loss            mean/std/max/min\t     0.004945\t    0.001567\t    0.011850\t  0.00152272\n",
      "EP Qmax            mean/std/max/min\t     1.872947\t    0.084076\t    2.149750\t     1.57615\n",
      "EP Test Reward     mean/std/max/min\t    24.280142\t   31.715763\t  305.000000\t         0.0\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   210\t Steps:    2110368\t Average Speed:  2146.67\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward mean/std/max/min\t     8.227000\t    7.190095\t   42.000000\t         0.0\n",
      "EP Loss            mean/std/max/min\t     0.006561\t    0.002243\t    0.016207\t  0.00221082\n",
      "EP Qmax            mean/std/max/min\t     1.879957\t    0.114261\t    2.211938\t     1.57516\n",
      "EP Test Reward     mean/std/max/min\t    25.298951\t   33.365215\t  305.000000\t         0.0\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   220\t Steps:    2210208\t Average Speed:  2136.33\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward mean/std/max/min\t     9.213000\t    7.978824\t   43.000000\t         0.0\n",
      "EP Loss            mean/std/max/min\t     0.006644\t    0.002189\t    0.015619\t  0.00202645\n",
      "EP Qmax            mean/std/max/min\t     1.944098\t    0.098260\t    2.281055\t     1.57065\n",
      "EP Test Reward     mean/std/max/min\t    28.424448\t   39.911530\t  305.000000\t         0.0\n",
      "====================================================================================================\n",
      "                                                                                                    \n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # args = parse_arguments()\n",
    "    # game = args.game\n",
    "    game = \"Breakout\"\n",
    "    train(game)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plt.plot(TRs)\n",
    "# device = torch.device('cuda:0')\n",
    "# replay = deque(maxlen=10000)\n",
    "# datas = [np.random.randn(1000, 32)] * 5\n",
    "# for entry in zip(*datas):\n",
    "#     replay.append(entry)\n",
    "#     \n",
    "# dataset = ReplayDataset(replay)\n",
    "# dataloader = DataLoaderX(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "# print(next(iter(dataloader)))\n",
    "# datafetcher = DataPrefetcher(dataloader, device)\n",
    "# datafetcher.preload()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# LLs = []\n",
    "# RRs = []\n",
    "# for epoch in range(10):\n",
    "#     model, model_target, Ls = train(model, model_target)\n",
    "#     # Rs = test(model)\n",
    "#     LLs += Ls\n",
    "#     # RRs += Rs\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import torch\n",
    "# data = torch.load('././ckpt/Breakout_e500')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# \n",
    "# for k, v in data.items():\n",
    "#     if k != 'model':\n",
    "#         if isinstance(v, list):\n",
    "#             if len(v) > 0:\n",
    "#                 plt.plot(v, label=k)\n",
    "# \n",
    "# plt.legend()\n",
    "# data.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 9), dpi=300)\n",
    "# plt.plot(data['Rs'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}