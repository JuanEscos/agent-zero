{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import cv2\n",
    "import copy\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import scipy\n",
    "import argparse\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import ray\n",
    "from scipy.signal import savgol_filter\n",
    "# plt.style.use('')\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "import torchvision as tv\n",
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "\n",
    "from src.common.atari_wrappers import wrap_deepmind, make_atari\n",
    "from src.common.utils import LinearSchedule, DataLoaderX, DataPrefetcher, ReplayDataset\n",
    "from src.common.vec_env import ShmemVecEnv, VecEnvWrapper, DummyVecEnv\n",
    "from src.agents.model import NatureCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "num_env = 16\n",
    "num_actors = 8\n",
    "total_steps = int(1e7)\n",
    "epoches = 1000\n",
    "update_per_data = 8\n",
    "replay_size = int(1e6)\n",
    "discount = 0.99\n",
    "batch_size = 512\n",
    "lr = 1e-3\n",
    "\n",
    "target_net_update_freq = 250\n",
    "exploration_ratio = 0.2\n",
    "steps_per_epoch = total_steps // epoches \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def make_env(game, episode_life=True, clip_rewards=True):\n",
    "    env = make_atari(f'{game}NoFrameskip-v4')\n",
    "    env = wrap_deepmind(env, episode_life=episode_life, clip_rewards=clip_rewards, frame_stack=True, scale=False, transpose_image=True)\n",
    "    return env\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--game\", type=str, default=\"Breakout\")\n",
    "    parser.add_argument(\"--replay_size\", type=int, default=int(1e6))\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1024)\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-4)\n",
    "    parser.add_argument(\"--gpu_id\", type=int, default=9)\n",
    "    parser.add_argument(\"--discount\", type=float, default=0.99)\n",
    "    parser.add_argument(\"--exploration_steps\", type=int, default=20000)\n",
    "    parser.add_argument(\"--max_step\", type=int, default=int(1e7))\n",
    "    parser.add_argument(\"--seed\", type=int, default=1234)\n",
    "    parser.add_argument(\"--num_env\", type=int, default=16)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    print(\"input args:\\n\", json.dumps(vars(args), indent=4, separators=(\",\", \":\")))\n",
    "    return args\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus=0.125)\n",
    "class Actor:\n",
    "    def __init__(self, rank, game):\n",
    "        if rank < num_actors:\n",
    "            self.envs = ShmemVecEnv([lambda: make_env(game) for _ in range(num_env)], context='fork')\n",
    "        else:\n",
    "            self.envs = ShmemVecEnv([lambda: make_env(game, True, True) for _ in range(num_env)], context='fork')\n",
    "        self.R = np.zeros(num_env)\n",
    "        self.obs = self.envs.reset()\n",
    "        self.state_shape, self.action_dim = self.envs.observation_space.shape, self.envs.action_space.n\n",
    "        self.model = NatureCNN(self.state_shape[0], self.action_dim).cuda()\n",
    "        self.rank = rank\n",
    "    \n",
    "    def sample(self, epsilon, state_dict):\n",
    "        self.model.load_state_dict(state_dict)\n",
    "        steps = steps_per_epoch // (num_env * num_actors)\n",
    "        Rs, Qs = [], []\n",
    "        tic = time.time()\n",
    "        local_replay = deque(maxlen=replay_size)\n",
    "        for step in range(steps):\n",
    "            action_random = np.random.randint(0, self.action_dim, num_env)\n",
    "            st = torch.from_numpy(np.array(self.obs)).float().cuda() / 255.0\n",
    "            qs = self.model(st)\n",
    "            qs_max, qs_argmax = qs.max(dim=-1)\n",
    "            action_greedy = qs_argmax.tolist()\n",
    "            Qs.append(qs_max.mean().item())\n",
    "            action = [act_grd if p > epsilon else act_rnd for p, act_rnd, act_grd in zip(np.random.rand(num_env), action_random, action_greedy)]\n",
    "    \n",
    "            obs_next, reward, done, info = self.envs.step(action)\n",
    "            for entry in zip(self.obs, action, reward, obs_next, done):\n",
    "                local_replay.append(entry)\n",
    "            self.obs = obs_next\n",
    "            self.R += np.array(reward)\n",
    "            for idx, d in enumerate(done):\n",
    "                if d:\n",
    "                    Rs.append(self.R[idx])\n",
    "                    self.R[idx] = 0\n",
    "        toc = time.time()\n",
    "        # print(f\"Rank {self.rank}, Data Collection Time: {toc - tic}, Speed {steps_per_epoch / (toc - tic)}\")\n",
    "        return local_replay, Rs, Qs, self.rank\n",
    "\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, game):\n",
    "        test_env = make_env(game)\n",
    "        self.state_shape, self.action_dim = test_env.observation_space.shape, test_env.action_space.n\n",
    "        self.model = NatureCNN(self.state_shape[0], self.action_dim).cuda()\n",
    "        self.model_target = copy.deepcopy(self.model).cuda()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
    "        self.replay = deque(maxlen=replay_size)\n",
    "        self.update_steps = 0\n",
    "        self.device = torch.device('cuda:0')\n",
    "        \n",
    "    def get_datafetcher(self):\n",
    "        dataset = ReplayDataset(self.replay)\n",
    "        dataloader = DataLoaderX(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        datafetcher = DataPrefetcher(dataloader, self.device)\n",
    "        return datafetcher\n",
    "    \n",
    "    def append_data(self, data):\n",
    "        self.replay.extend(data)\n",
    "    \n",
    "    def train_step(self):\n",
    "        try:\n",
    "            data = self.prefetcher.next()\n",
    "        except:\n",
    "            self.prefetcher = self.get_datafetcher()\n",
    "            data = self.prefetcher.next()\n",
    "\n",
    "        states, actions, rewards, next_states, terminals = data\n",
    "        states = states.float() / 255.0\n",
    "        next_states = next_states.float() / 255.0\n",
    "        actions = actions.long()\n",
    "        terminals = terminals.float()\n",
    "        rewards = rewards.float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_next = self.model_target(next_states)\n",
    "            q_next_online = self.model(next_states)\n",
    "            q_next = q_next.gather(1, q_next_online.argmax(dim=-1).unsqueeze(-1)).squeeze(-1)\n",
    "            q_target = rewards + discount * (1 - terminals) * q_next\n",
    "\n",
    "        q = self.model(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "        loss = F.smooth_l1_loss(q, q_target)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.update_steps += 1\n",
    "\n",
    "        if self.update_steps % 250 == 0:\n",
    "            self.model_target.load_state_dict(self.model.state_dict())\n",
    "        return loss.detach()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def formated_print(var_name, xs):\n",
    "    if len(xs) > 0:\n",
    "        print(\"{0} mean/std/max/min\\t {1:12.6f}\\t{2:12.6f}\\t{3:12.6f}\\t\".format(var_name, np.mean(xs), np.std(xs), np.max(xs), np.min(xs)))\n",
    "\n",
    "def train(game):\n",
    "    ray.init()\n",
    "    \n",
    "    epsilon_schedule = LinearSchedule(1.0, 0.01, int(total_steps * exploration_ratio))\n",
    "    actors = [Actor.remote(rank, game) for rank in range(num_actors)]\n",
    "    tester = Actor.remote(num_actors, game)\n",
    "    \n",
    "    agent = Agent(game)\n",
    "    sample_ops = [a.sample.remote(1.0, agent.model.state_dict()) for a in actors]\n",
    "    test_op = tester.sample.remote(0.01, agent.model.state_dict())\n",
    "    \n",
    "    TRRs, RRs, QQs, LLs = [], [], [], []\n",
    "    for local_replay, Rs, Qs, rank in ray.get(sample_ops + [test_op]):\n",
    "        if rank < num_actors:\n",
    "            agent.append_data(local_replay)\n",
    "            RRs += Rs\n",
    "            QQs += Qs\n",
    "        else:\n",
    "            TRRs += Rs\n",
    "            \n",
    "    \n",
    "    formated_print(\"Warming up Reward\", RRs)\n",
    "    formated_print(\"Warming up Qmax\", QQs)\n",
    "        \n",
    "    steps = 0\n",
    "    epoch = 0\n",
    "    tic = time.time()\n",
    "    while True:\n",
    "        done_id, sample_ops = ray.wait(sample_ops)\n",
    "        data = ray.get(done_id)\n",
    "        local_replay, Rs, Qs, rank = data[0]\n",
    "        \n",
    "        if rank < num_actors:\n",
    "            # Actor\n",
    "            agent.append_data(local_replay)\n",
    "            steps += len(local_replay)\n",
    "            epsilon = epsilon_schedule(len(local_replay))\n",
    "            sample_ops.append(actors[rank].sample.remote(epsilon, agent.model.state_dict()))\n",
    "            RRs += Rs\n",
    "            QQs += Qs\n",
    "        else:\n",
    "            # Tester\n",
    "            sample_ops.append(tester.sample.remote(0.01, agent.model.state_dict()))\n",
    "            TRRs += Rs\n",
    "        \n",
    "        # Trainer\n",
    "        for _ in range(16):\n",
    "            loss = agent.train_step()\n",
    "            LLs.append(loss)\n",
    "        \n",
    "        if (steps // steps_per_epoch) > epoch:\n",
    "            if epoch % 10 == 0:\n",
    "                toc = time.time()\n",
    "                print(\"=\" * 100)\n",
    "                print(f\"Epoch: {epoch:5d}\\t Steps: {steps:10d}\\t Average Speed: {steps / (toc - tic):8.2f}\\t Epsilon: {epsilon}\")\n",
    "                print('-' * 100)\n",
    "                formated_print(\"EP Training Reward\", RRs[-1000:])\n",
    "                formated_print(\"EP Loss           \", torch.stack(LLs).tolist()[-1000:])\n",
    "                formated_print(\"EP Qmax           \", QQs[-1000:])\n",
    "                if len(TRRs) > 0:\n",
    "                    formated_print(\"EP Test Reward    \", TRRs[-1000:])\n",
    "                    \n",
    "                print(\"=\" * 100)\n",
    "                print(\" \" * 100)\n",
    "                \n",
    "                torch.save({\n",
    "                    'model': agent.model.state_dict(),\n",
    "                    'optim': agent.optimizer.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'epsilon': epsilon,\n",
    "                    'steps': steps,\n",
    "                    'Rs': RRs,\n",
    "                    'TRs': TRRs,\n",
    "                    'Qs': QQs,\n",
    "                    'Ls': LLs,\n",
    "                }, f'ckpt/{game}_e{epoch}')\n",
    "            \n",
    "            epoch += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "2020-08-03 07:09:15,532\tINFO resource_spec.py:212 -- Starting Ray with 231.15 GiB memory available for workers and up to 103.06 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-08-03 07:09:15,805\tWARNING services.py:923 -- Redis failed to start, retrying now.\n",
      "2020-08-03 07:09:16,177\tINFO services.py:1165 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n",
      "/home/bzhou/miniconda3/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "\u001b[2m\u001b[36m(pid=11028)\u001b[0m Logging to /tmp/openai-2020-08-03-07-09-18-002032\n",
      "\u001b[2m\u001b[36m(pid=11028)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=11030)\u001b[0m Logging to /tmp/openai-2020-08-03-07-09-17-981485\n",
      "\u001b[2m\u001b[36m(pid=11030)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=11057)\u001b[0m Logging to /tmp/openai-2020-08-03-07-09-18-002557\n",
      "\u001b[2m\u001b[36m(pid=11057)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=11072)\u001b[0m Logging to /tmp/openai-2020-08-03-07-09-17-993887\n",
      "\u001b[2m\u001b[36m(pid=11072)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=11076)\u001b[0m Logging to /tmp/openai-2020-08-03-07-09-17-982181\n",
      "\u001b[2m\u001b[36m(pid=11076)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=11086)\u001b[0m Logging to /tmp/openai-2020-08-03-07-09-17-996747\n",
      "\u001b[2m\u001b[36m(pid=11086)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=11067)\u001b[0m Logging to /tmp/openai-2020-08-03-07-09-17-984156\n",
      "\u001b[2m\u001b[36m(pid=11067)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=11037)\u001b[0m Logging to /tmp/openai-2020-08-03-07-09-18-001882\n",
      "\u001b[2m\u001b[36m(pid=11037)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=11063)\u001b[0m Logging to /tmp/openai-2020-08-03-07-09-18-009260\n",
      "\u001b[2m\u001b[36m(pid=11063)\u001b[0m Creating dummy env object to get spaces\n",
      "Warming up reward: 0.12186379928315412 0.3271284361033587 1.0\n",
      "Warming up Qmax: -0.13257039826697645 0.005888359608290227 -0.11684755980968475\n",
      "====================================================================================================\n",
      "Epoch:     0\t Steps:      11232\t Average Speed:  1574.28\t Epsilon: 0.9950579199999998\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     0.153846\t    0.396148\t    3.000000\t\n",
      "EP Loss           \t     0.032107\t    0.284610\t    3.387205\t\n",
      "EP Qmax           \t    -0.131465\t    0.008262\t   -0.113120\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    10\t Steps:     111072\t Average Speed:  2510.18\t Epsilon: 0.945637119999998\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     0.295000\t    0.629265\t    3.000000\t\n",
      "EP Loss           \t     0.001187\t    0.000742\t    0.005123\t\n",
      "EP Qmax           \t    -0.063723\t    0.041392\t    0.137294\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    20\t Steps:     210912\t Average Speed:  2676.70\t Epsilon: 0.8962163199999962\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     0.274000\t    0.604089\t    3.000000\t\n",
      "EP Loss           \t     0.000814\t    0.000545\t    0.004626\t\n",
      "EP Qmax           \t    -0.008980\t    0.057082\t    0.223163\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    30\t Steps:     310752\t Average Speed:  2769.34\t Epsilon: 0.8467955199999944\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     0.247000\t    0.598323\t    4.000000\t\n",
      "EP Loss           \t     0.000707\t    0.000440\t    0.003460\t\n",
      "EP Qmax           \t     0.044429\t    0.075585\t    0.330473\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    40\t Steps:     410592\t Average Speed:  2843.27\t Epsilon: 0.7973747199999925\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     0.250000\t    0.547266\t    3.000000\t\n",
      "EP Loss           \t     0.001010\t    0.000500\t    0.003674\t\n",
      "EP Qmax           \t     0.107755\t    0.081710\t    0.389278\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    50\t Steps:     510432\t Average Speed:  2906.31\t Epsilon: 0.7479539199999907\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     0.401000\t    0.701569\t    4.000000\t\n",
      "EP Loss           \t     0.005348\t    0.020951\t    0.468372\t\n",
      "EP Qmax           \t     0.257505\t    0.103758\t    0.633580\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    60\t Steps:     610272\t Average Speed:  2942.52\t Epsilon: 0.6985331199999889\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     0.654000\t    0.840407\t    5.000000\t\n",
      "EP Loss           \t     0.003553\t    0.001245\t    0.009954\t\n",
      "EP Qmax           \t     0.461798\t    0.098962\t    0.850547\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    70\t Steps:     710112\t Average Speed:  2963.57\t Epsilon: 0.6491123199999871\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     0.863000\t    1.024808\t    6.000000\t\n",
      "EP Loss           \t     0.003419\t    0.001081\t    0.008668\t\n",
      "EP Qmax           \t     0.581906\t    0.087162\t    0.922678\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    80\t Steps:     811200\t Average Speed:  2973.45\t Epsilon: 0.5990737599999852\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     0.891000\t    1.155473\t    6.000000\t\n",
      "EP Loss           \t     0.003405\t    0.000934\t    0.007710\t\n",
      "EP Qmax           \t     0.660185\t    0.086977\t    0.924383\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    90\t Steps:     911040\t Average Speed:  2988.44\t Epsilon: 0.5496529599999834\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     1.013000\t    1.271547\t    6.000000\t\n",
      "EP Loss           \t     0.003225\t    0.000824\t    0.006642\t\n",
      "EP Qmax           \t     0.696112\t    0.084338\t    0.956191\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   100\t Steps:    1010880\t Average Speed:  2970.48\t Epsilon: 0.5002321599999816\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     1.125000\t    1.351064\t    7.000000\t\n",
      "EP Loss           \t     0.003666\t    0.003747\t    0.087314\t\n",
      "EP Qmax           \t     0.764269\t    0.084874\t    1.009705\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   110\t Steps:    1110720\t Average Speed:  2980.67\t Epsilon: 0.4508113599999798\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     1.416000\t    1.465928\t    7.000000\t\n",
      "EP Loss           \t     0.005918\t    0.003580\t    0.045241\t\n",
      "EP Qmax           \t     0.780517\t    0.074851\t    1.024295\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   120\t Steps:    1210560\t Average Speed:  2959.58\t Epsilon: 0.40139055999997797\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     1.707000\t    1.686165\t   13.000000\t\n",
      "EP Loss           \t     0.003630\t    0.001026\t    0.012669\t\n",
      "EP Qmax           \t     0.842070\t    0.088098\t    1.107147\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   130\t Steps:    1310400\t Average Speed:  2955.25\t Epsilon: 0.35196975999997615\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     1.806000\t    1.857516\t   12.000000\t\n",
      "EP Loss           \t     0.003869\t    0.001276\t    0.013612\t\n",
      "EP Qmax           \t     0.928112\t    0.092717\t    1.194583\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   140\t Steps:    1410240\t Average Speed:  2950.89\t Epsilon: 0.30254895999997433\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     2.248000\t    2.008605\t   13.000000\t\n",
      "EP Loss           \t     0.004062\t    0.001045\t    0.007771\t\n",
      "EP Qmax           \t     1.013223\t    0.108468\t    1.337426\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   150\t Steps:    1510080\t Average Speed:  2939.62\t Epsilon: 0.2531281599999725\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     2.647000\t    2.203268\t   15.000000\t\n",
      "EP Loss           \t     0.004332\t    0.001011\t    0.008516\t\n",
      "EP Qmax           \t     1.086403\t    0.092687\t    1.373501\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   160\t Steps:    1611168\t Average Speed:  2929.88\t Epsilon: 0.20308959999997278\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     3.147000\t    2.424746\t   14.000000\t\n",
      "EP Loss           \t     0.004637\t    0.001128\t    0.009723\t\n",
      "EP Qmax           \t     1.191764\t    0.099515\t    1.499921\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   170\t Steps:    1711008\t Average Speed:  2926.65\t Epsilon: 0.15366879999997318\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     3.931000\t    3.018317\t   19.000000\t\n",
      "EP Loss           \t     0.005307\t    0.001351\t    0.011014\t\n",
      "EP Qmax           \t     1.274618\t    0.096699\t    1.608484\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   180\t Steps:    1810848\t Average Speed:  2921.08\t Epsilon: 0.10424799999997358\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     4.731000\t    3.552835\t   22.000000\t\n",
      "EP Loss           \t     0.004845\t    0.001232\t    0.011265\t\n",
      "EP Qmax           \t     1.324283\t    0.083071\t    1.540149\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   190\t Steps:    1910688\t Average Speed:  2907.48\t Epsilon: 0.054827199999973895\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     5.444000\t    4.073679\t   23.000000\t\n",
      "EP Loss           \t     0.005017\t    0.001364\t    0.010175\t\n",
      "EP Qmax           \t     1.380856\t    0.089357\t    1.692336\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   200\t Steps:    2010528\t Average Speed:  2899.23\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     6.402000\t    4.812733\t   40.000000\t\n",
      "EP Loss           \t     0.005661\t    0.001567\t    0.011781\t\n",
      "EP Qmax           \t     1.421352\t    0.087732\t    1.685989\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   210\t Steps:    2110368\t Average Speed:  2885.76\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     7.307000\t    5.933865\t   40.000000\t\n",
      "EP Loss           \t     0.005572\t    0.001428\t    0.011691\t\n",
      "EP Qmax           \t     1.487956\t    0.090156\t    1.727210\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   220\t Steps:    2210208\t Average Speed:  2869.13\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     8.284000\t    6.991662\t   41.000000\t\n",
      "EP Loss           \t     0.006045\t    0.001638\t    0.012281\t\n",
      "EP Qmax           \t     1.510919\t    0.100846\t    1.837096\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   230\t Steps:    2310048\t Average Speed:  2842.22\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     8.599000\t    7.655599\t   41.000000\t\n",
      "EP Loss           \t     0.006639\t    0.001749\t    0.012989\t\n",
      "EP Qmax           \t     1.573285\t    0.095962\t    1.876725\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   240\t Steps:    2411136\t Average Speed:  2826.46\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     9.265000\t    8.431179\t   48.000000\t\n",
      "EP Loss           \t     0.006673\t    0.001769\t    0.013201\t\n",
      "EP Qmax           \t     1.632057\t    0.096256\t    1.922424\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   250\t Steps:    2510976\t Average Speed:  2817.74\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     9.635000\t    8.855155\t   58.000000\t\n",
      "EP Loss           \t     0.007455\t    0.002018\t    0.015778\t\n",
      "EP Qmax           \t     1.746995\t    0.094076\t    1.997505\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   260\t Steps:    2610816\t Average Speed:  2819.94\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    10.250000\t    9.428971\t   74.000000\t\n",
      "EP Loss           \t     0.007228\t    0.002112\t    0.018006\t\n",
      "EP Qmax           \t     1.760590\t    0.092818\t    2.007807\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   270\t Steps:    2710656\t Average Speed:  2808.93\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    10.709000\t    9.999916\t   74.000000\t\n",
      "EP Loss           \t     0.007531\t    0.002095\t    0.016394\t\n",
      "EP Qmax           \t     1.844268\t    0.102946\t    2.144085\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   280\t Steps:    2810496\t Average Speed:  2804.05\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    11.054000\t   10.573509\t   75.000000\t\n",
      "EP Loss           \t     0.009088\t    0.002458\t    0.020422\t\n",
      "EP Qmax           \t     1.928855\t    0.128708\t    2.349649\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   290\t Steps:    2910336\t Average Speed:  2799.01\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    11.618000\t   10.959930\t   75.000000\t\n",
      "EP Loss           \t     0.010917\t    0.003018\t    0.024872\t\n",
      "EP Qmax           \t     2.006745\t    0.106607\t    2.352859\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   300\t Steps:    3010176\t Average Speed:  2785.45\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    12.033000\t   11.411131\t   75.000000\t\n",
      "EP Loss           \t     0.011579\t    0.003081\t    0.023925\t\n",
      "EP Qmax           \t     2.102387\t    0.118558\t    2.535160\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   310\t Steps:    3110016\t Average Speed:  2779.13\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    12.301000\t   11.931488\t   69.000000\t\n",
      "EP Loss           \t     0.012983\t    0.003578\t    0.029971\t\n",
      "EP Qmax           \t     2.169380\t    0.148548\t    2.658744\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   320\t Steps:    3211104\t Average Speed:  2764.44\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    12.353000\t   12.203212\t   69.000000\t\n",
      "EP Loss           \t     0.014615\t    0.004202\t    0.032623\t\n",
      "EP Qmax           \t     2.267524\t    0.135550\t    2.857624\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   330\t Steps:    3310944\t Average Speed:  2750.42\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    11.794000\t   12.482210\t   77.000000\t\n",
      "EP Loss           \t     0.015381\t    0.004214\t    0.034373\t\n",
      "EP Qmax           \t     2.413444\t    0.153704\t    2.835342\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   340\t Steps:    3410784\t Average Speed:  2733.04\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    11.628000\t   13.726675\t   81.000000\t\n",
      "EP Loss           \t     0.017979\t    0.005460\t    0.046321\t\n",
      "EP Qmax           \t     2.401711\t    0.196606\t    3.061234\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   350\t Steps:    3510624\t Average Speed:  2732.26\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    11.313000\t   14.140121\t   93.000000\t\n",
      "EP Loss           \t     0.020199\t    0.005830\t    0.045617\t\n",
      "EP Qmax           \t     2.545677\t    0.205400\t    3.325207\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   360\t Steps:    3610464\t Average Speed:  2724.49\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    11.445000\t   14.976614\t   93.000000\t\n",
      "EP Loss           \t     0.022037\t    0.007045\t    0.059053\t\n",
      "EP Qmax           \t     2.610357\t    0.264821\t    3.824454\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   370\t Steps:    3710304\t Average Speed:  2720.91\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    11.016000\t   14.874533\t   93.000000\t\n",
      "EP Loss           \t     0.025967\t    0.008062\t    0.061624\t\n",
      "EP Qmax           \t     2.586198\t    0.239243\t    3.276716\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   380\t Steps:    3810144\t Average Speed:  2715.94\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    10.282000\t   15.017273\t   89.000000\t\n",
      "EP Loss           \t     0.027447\t    0.008670\t    0.069147\t\n",
      "EP Qmax           \t     2.705904\t    0.219561\t    3.490125\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   390\t Steps:    3911232\t Average Speed:  2707.00\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     9.814000\t   14.716229\t   90.000000\t\n",
      "EP Loss           \t     0.029147\t    0.009139\t    0.065121\t\n",
      "EP Qmax           \t     2.740670\t    0.256309\t    3.582832\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   400\t Steps:    4011072\t Average Speed:  2703.66\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     9.765000\t   14.564676\t   90.000000\t\n",
      "EP Loss           \t     0.031647\t    0.009541\t    0.069940\t\n",
      "EP Qmax           \t     2.781893\t    0.206843\t    3.619850\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   410\t Steps:    4110912\t Average Speed:  2692.16\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    10.994000\t   14.906843\t   90.000000\t\n",
      "EP Loss           \t     0.031676\t    0.009925\t    0.087700\t\n",
      "EP Qmax           \t     3.067585\t    0.294850\t    3.948147\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   420\t Steps:    4210752\t Average Speed:  2684.78\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    11.353000\t   14.633673\t   83.000000\t\n",
      "EP Loss           \t     0.033181\t    0.010136\t    0.083390\t\n",
      "EP Qmax           \t     3.167297\t    0.326263\t    4.267260\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   430\t Steps:    4310592\t Average Speed:  2679.08\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    11.040000\t   14.492219\t   81.000000\t\n",
      "EP Loss           \t     0.033082\t    0.009862\t    0.078007\t\n",
      "EP Qmax           \t     3.057098\t    0.291420\t    4.042565\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   440\t Steps:    4410432\t Average Speed:  2673.62\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    10.158000\t   14.205740\t   81.000000\t\n",
      "EP Loss           \t     0.031590\t    0.009547\t    0.075255\t\n",
      "EP Qmax           \t     3.143620\t    0.246728\t    4.048688\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   450\t Steps:    4510272\t Average Speed:  2668.62\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     9.576000\t   13.252933\t   80.000000\t\n",
      "EP Loss           \t     0.030907\t    0.009567\t    0.076126\t\n",
      "EP Qmax           \t     3.182255\t    0.188884\t    4.048618\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   460\t Steps:    4610112\t Average Speed:  2665.89\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    10.275000\t   14.415387\t   92.000000\t\n",
      "EP Loss           \t     0.029442\t    0.008564\t    0.066062\t\n",
      "EP Qmax           \t     3.390213\t    0.291776\t    4.359647\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   470\t Steps:    4711200\t Average Speed:  2658.83\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    10.318000\t   14.211998\t   92.000000\t\n",
      "EP Loss           \t     0.029200\t    0.008986\t    0.078293\t\n",
      "EP Qmax           \t     3.759336\t    0.450543\t    5.529331\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   480\t Steps:    4811040\t Average Speed:  2654.46\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    12.283000\t   17.836169\t   92.000000\t\n",
      "EP Loss           \t     0.031719\t    0.009024\t    0.066853\t\n",
      "EP Qmax           \t     3.644764\t    0.551231\t    5.869682\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   490\t Steps:    4910880\t Average Speed:  2651.35\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    12.513000\t   18.382541\t   92.000000\t\n",
      "EP Loss           \t     0.033676\t    0.009581\t    0.080624\t\n",
      "EP Qmax           \t     3.464170\t    0.257037\t    4.191469\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   500\t Steps:    5010720\t Average Speed:  2645.46\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    12.698000\t   18.646147\t   92.000000\t\n",
      "EP Loss           \t     0.035396\t    0.010293\t    0.082949\t\n",
      "EP Qmax           \t     3.906781\t    0.298863\t    4.778687\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   510\t Steps:    5110560\t Average Speed:  2642.99\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    12.562000\t   18.065496\t   92.000000\t\n",
      "EP Loss           \t     0.035200\t    0.010397\t    0.082343\t\n",
      "EP Qmax           \t     3.875914\t    0.305548\t    4.795672\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   520\t Steps:    5210400\t Average Speed:  2640.52\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    13.423000\t   18.602475\t   87.000000\t\n",
      "EP Loss           \t     0.035136\t    0.010625\t    0.103735\t\n",
      "EP Qmax           \t     3.988623\t    0.490076\t    5.241069\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   530\t Steps:    5310240\t Average Speed:  2636.76\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    13.998000\t   18.854867\t   87.000000\t\n",
      "EP Loss           \t     0.035498\t    0.010674\t    0.082176\t\n",
      "EP Qmax           \t     4.157417\t    0.473831\t    5.269034\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   540\t Steps:    5410080\t Average Speed:  2630.56\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    14.659000\t   19.710219\t   98.000000\t\n",
      "EP Loss           \t     0.038431\t    0.010903\t    0.080277\t\n",
      "EP Qmax           \t     3.984466\t    0.461554\t    5.234198\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # args = parse_arguments()\n",
    "    # game = args.game\n",
    "    game = \"Breakout\"\n",
    "    train(game)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plt.plot(TRs)\n",
    "# device = torch.device('cuda:0')\n",
    "# replay = deque(maxlen=10000)\n",
    "# datas = [np.random.randn(1000, 32)] * 5\n",
    "# for entry in zip(*datas):\n",
    "#     replay.append(entry)\n",
    "#     \n",
    "# dataset = ReplayDataset(replay)\n",
    "# dataloader = DataLoaderX(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "# print(next(iter(dataloader)))\n",
    "# datafetcher = DataPrefetcher(dataloader, device)\n",
    "# datafetcher.preload()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# LLs = []\n",
    "# RRs = []\n",
    "# for epoch in range(10):\n",
    "#     model, model_target, Ls = train(model, model_target)\n",
    "#     # Rs = test(model)\n",
    "#     LLs += Ls\n",
    "#     # RRs += Rs\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plt.plot(Ls)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('replay.pkl', 'wb') as f:\n",
    "#     pickle.dump(replay, f, pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}