{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import cv2\n",
    "import copy\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import scipy\n",
    "import argparse\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import ray\n",
    "from scipy.signal import savgol_filter\n",
    "# plt.style.use('')\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "import torchvision as tv\n",
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "\n",
    "from src.common.atari_wrappers import wrap_deepmind, make_atari\n",
    "from src.common.utils import LinearSchedule, DataLoaderX, DataPrefetcher, ReplayDataset\n",
    "from src.common.vec_env import ShmemVecEnv, VecEnvWrapper, DummyVecEnv\n",
    "from src.agents.model import NatureCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "num_env = 16\n",
    "num_actors = 8\n",
    "total_steps = int(1e7)\n",
    "epoches = 1000\n",
    "update_per_data = 8\n",
    "replay_size = int(1e6)\n",
    "discount = 0.99\n",
    "batch_size = 512\n",
    "lr = 1e-3\n",
    "\n",
    "target_net_update_freq = 200\n",
    "exploration_ratio = 0.1\n",
    "steps_per_epoch = total_steps // epoches \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def make_env(game, episode_life=True, clip_rewards=True):\n",
    "    env = make_atari(f'{game}NoFrameskip-v4')\n",
    "    env = wrap_deepmind(env, episode_life=episode_life, clip_rewards=clip_rewards, frame_stack=True, scale=False, transpose_image=True)\n",
    "    return env"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus=0.125)\n",
    "class Actor:\n",
    "    def __init__(self, rank, game):\n",
    "        if rank < num_actors:\n",
    "            self.envs = ShmemVecEnv([lambda: make_env(game) for _ in range(num_env)], context='fork')\n",
    "        else:\n",
    "            self.envs = ShmemVecEnv([lambda: make_env(game, False, False) for _ in range(num_env)], context='fork')\n",
    "        self.R = np.zeros(num_env)\n",
    "        self.obs = self.envs.reset()\n",
    "        self.state_shape, self.action_dim = self.envs.observation_space.shape[0], self.envs.action_space.n\n",
    "        self.model = NatureCNN(self.state_shape[0], self.action_dim).cuda()\n",
    "        self.rank = rank\n",
    "    \n",
    "    def sample(self, epsilon, state_dict):\n",
    "        self.model.load_state_dict(state_dict)\n",
    "        steps = steps_per_epoch // (num_env * num_actors)\n",
    "        Rs, Qs = [], []\n",
    "        tic = time.time()\n",
    "        local_replay = deque(maxlen=replay_size)\n",
    "        for step in range(steps):\n",
    "            action_random = np.random.randint(0, self.action_dim, num_env)\n",
    "            st = torch.from_numpy(np.array(self.obs)).float().cuda() / 255.0\n",
    "            qs = self.model(st)\n",
    "            qs_max, qs_argmax = qs.max(dim=-1)\n",
    "            action_greedy = qs_argmax.tolist()\n",
    "            Qs.append(qs_max.mean().item())\n",
    "            action = [act_grd if p > epsilon else act_rnd for p, act_rnd, act_grd in zip(np.random.rand(num_env), action_random, action_greedy)]\n",
    "    \n",
    "            obs_next, reward, done, info = self.envs.step(action)\n",
    "            for entry in zip(self.obs, action, reward, obs_next, done):\n",
    "                local_replay.append(entry)\n",
    "            self.obs = obs_next\n",
    "            self.R += np.array(reward)\n",
    "            for idx, d in enumerate(done):\n",
    "                if d:\n",
    "                    Rs.append(self.R[idx])\n",
    "                    self.R[idx] = 0\n",
    "        toc = time.time()\n",
    "        # print(f\"Rank {self.rank}, Data Collection Time: {toc - tic}, Speed {steps_per_epoch / (toc - tic)}\")\n",
    "        return local_replay, Rs, Qs, self.rank\n",
    "\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, game):\n",
    "        test_env = make_atari(game)\n",
    "        self.state_shape, self.action_dim = test_env.observation_space.shape[0], test_env.action_space.n\n",
    "        self.model = NatureCNN(self.state_shape[0], self.action_dim).cuda()\n",
    "        self.model_target = copy.deepcopy(self.model).cuda()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
    "        self.replay = deque(maxlen=replay_size)\n",
    "        self.update_steps = 0\n",
    "        self.device = torch.device('cuda:0')\n",
    "        \n",
    "    def get_datafetcher(self):\n",
    "        dataset = ReplayDataset(self.replay)\n",
    "        dataloader = DataLoaderX(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        datafetcher = DataPrefetcher(dataloader, self.device)\n",
    "        datafetcher.preload()\n",
    "        return datafetcher\n",
    "    \n",
    "    def append_data(self, data):\n",
    "        self.replay.extend(data)\n",
    "    \n",
    "    def train_step(self):\n",
    "        try:\n",
    "            data = self.prefetcher.next()\n",
    "        except:\n",
    "            self.prefetcher = self.get_datafetcher()\n",
    "            data = self.prefetcher.next()\n",
    "\n",
    "        states, actions, rewards, next_states, terminals = data\n",
    "        states = states.float() / 255.0\n",
    "        next_states = next_states.float() / 255.0\n",
    "        actions = actions.long()\n",
    "        terminals = terminals.float()\n",
    "        rewards = rewards.float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_next = self.model_target(next_states)\n",
    "            q_next_online = self.model(next_states)\n",
    "            q_next = q_next.gather(1, q_next_online.argmax(dim=-1).unsqueeze(-1)).squeeze(-1)\n",
    "            q_target = rewards + discount * (1 - terminals) * q_next\n",
    "\n",
    "        q = self.model(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "        loss = F.smooth_l1_loss(q, q_target)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.update_steps += 1\n",
    "\n",
    "        if self.update_steps % 500 == 0:\n",
    "            self.model_target.load_state_dict(self.model.state_dict())\n",
    "        return loss.detach()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def formated_print(var_name, xs):\n",
    "    print(\"{0}\\t {1:10.5f}\\t{2:10.5f}\\t{3:10.5f}\\t\".format(var_name, np.mean(xs), np.std(xs), np.max(xs)))\n",
    "\n",
    "def train(game):\n",
    "    ray.init()\n",
    "    \n",
    "    epsilon_schedule = LinearSchedule(1.0, 0.01, int(total_steps * exploration_ratio))\n",
    "    actors = [Actor.remote(rank, game) for rank in range(num_actors)]\n",
    "    tester = Actor.remote(num_actors)\n",
    "    \n",
    "    agent = Agent(game)\n",
    "    sample_ops = [a.sample.remote(1.0, agent.model.state_dict()) for a in actors]\n",
    "    test_op = tester.sample.remote(0.01, agent.model.state_dict())\n",
    "    \n",
    "    TRRs, RRs, QQs, LLs = [], [], [], []\n",
    "    for local_replay, Rs, Qs, rank in ray.get(sample_ops + [test_op]):\n",
    "        if rank < num_actors:\n",
    "            agent.append_data(local_replay)\n",
    "            RRs += Rs\n",
    "            QQs += Qs\n",
    "        else:\n",
    "            TRRs += Rs\n",
    "            \n",
    "    \n",
    "    print(\"Warming up reward:\", np.mean(RRs), np.std(RRs), np.max(RRs))\n",
    "    print(\"Warming up Qmax:\", np.mean(QQs), np.std(QQs), np.max(QQs))        \n",
    "        \n",
    "    steps = 0\n",
    "    epoch = 0\n",
    "    tic = time.time()\n",
    "    while True:\n",
    "        done_id, sample_ops = ray.wait(sample_ops)\n",
    "        data = ray.get(done_id)\n",
    "        local_replay, Rs, Qs, rank = data[0]\n",
    "        \n",
    "        if rank < num_actors:\n",
    "            # Actor\n",
    "            agent.append_data(local_replay)\n",
    "            steps += len(local_replay)\n",
    "            epsilon = epsilon_schedule(len(local_replay))\n",
    "            sample_ops.append(actors[rank].sample.remote(epsilon, agent.model.state_dict()))\n",
    "            \n",
    "            RRs += Rs\n",
    "            QQs += Qs\n",
    "        else:\n",
    "            # Tester\n",
    "            sample_ops.append(tester.sample.remote(0.01, agent.model.state_dict()))\n",
    "            TRRs += Rs\n",
    "        \n",
    "        # Trainer\n",
    "        for _ in range(20):\n",
    "            loss = agent.train_step()\n",
    "            LLs.append(loss)\n",
    "        \n",
    "        if (steps // steps_per_epoch) > epoch:\n",
    "            if epoch % 10 == 0:\n",
    "                toc = time.time()\n",
    "                print(\"=\" * 100)\n",
    "                print(f\"Epoch: {epoch:5d}\\t Steps: {steps:10d}\\t Average Speed: {steps / (toc - tic):8.2f}\\t Epsilon: {epsilon}\")\n",
    "                print('-' * 100)\n",
    "                formated_print(\"EP Training Reward\", RRs[-1000:])\n",
    "                formated_print(\"EP Loss           \", torch.stack(LLs).tolist()[-1000:])\n",
    "                formated_print(\"EP Test Reward    \", TRRs[-1000:])\n",
    "                formated_print(\"EP Qmax           \", QQs[-1000:])\n",
    "    \n",
    "                print(\"=\" * 100)\n",
    "                print(\" \" * 100)\n",
    "                \n",
    "                torch.save({\n",
    "                    'model': agent.model.state_dict(),\n",
    "                    'optim': agent.optimizer.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'epsilon': epsilon,\n",
    "                    'steps': steps,\n",
    "                    'Rs': RRs,\n",
    "                    'TRs': TRRs,\n",
    "                    'Qs': QQs,\n",
    "                    'Ls': LLs,\n",
    "                }, f'ckpt/{game}_e{epoch}')\n",
    "\n",
    "            epoch += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "2020-08-03 05:26:20,060\tINFO resource_spec.py:212 -- Starting Ray with 231.01 GiB memory available for workers and up to 103.01 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-08-03 05:26:20,310\tWARNING services.py:923 -- Redis failed to start, retrying now.\n",
      "2020-08-03 05:26:20,681\tINFO services.py:1165 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n",
      "/home/bzhou/miniconda3/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n",
      "Exception in thread Thread-86:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bzhou/miniconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/bzhou/miniconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/bzhou/miniconda3/lib/python3.6/site-packages/torch/utils/data/_utils/pin_memory.py\", line 25, in _pin_memory_loop\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/bzhou/miniconda3/lib/python3.6/multiprocessing/queues.py\", line 113, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/bzhou/miniconda3/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 294, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/bzhou/miniconda3/lib/python3.6/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/home/bzhou/miniconda3/lib/python3.6/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/home/bzhou/miniconda3/lib/python3.6/multiprocessing/connection.py\", line 487, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/home/bzhou/miniconda3/lib/python3.6/multiprocessing/connection.py\", line 614, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "\u001b[2m\u001b[36m(pid=46936)\u001b[0m Logging to /tmp/openai-2020-08-03-05-26-22-628167\n",
      "\u001b[2m\u001b[36m(pid=46936)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=46948)\u001b[0m Logging to /tmp/openai-2020-08-03-05-26-22-626287\n",
      "\u001b[2m\u001b[36m(pid=46948)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=46952)\u001b[0m Logging to /tmp/openai-2020-08-03-05-26-22-615337\n",
      "\u001b[2m\u001b[36m(pid=46952)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=46962)\u001b[0m Logging to /tmp/openai-2020-08-03-05-26-22-646089\n",
      "\u001b[2m\u001b[36m(pid=46962)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=46960)\u001b[0m Logging to /tmp/openai-2020-08-03-05-26-22-625368\n",
      "\u001b[2m\u001b[36m(pid=46960)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=46954)\u001b[0m Logging to /tmp/openai-2020-08-03-05-26-22-614172\n",
      "\u001b[2m\u001b[36m(pid=46954)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=46956)\u001b[0m Logging to /tmp/openai-2020-08-03-05-26-22-639788\n",
      "\u001b[2m\u001b[36m(pid=46956)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=46958)\u001b[0m Logging to /tmp/openai-2020-08-03-05-26-22-635581\n",
      "\u001b[2m\u001b[36m(pid=46958)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=46955)\u001b[0m Logging to /tmp/openai-2020-08-03-05-26-22-694604\n",
      "\u001b[2m\u001b[36m(pid=46955)\u001b[0m Creating dummy env object to get spaces\n",
      "Warming up reward: 0.07746478873239436 0.2673275055789057 1.0\n",
      "Warming up Qmax: 0.34068546831034696 0.004024685989710357 0.3583836853504181\n",
      "====================================================================================================\n",
      "Epoch     0\t Steps      11232\t Average Speed  1509.49\t Epsilon: 0.9950579199999998\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    0.10465\t   0.33213\t   2.00000\t\n",
      "EP Loss           \t    0.02063\t   0.17631\t   2.35551\t\n",
      "EP Test Reward    \t    0.00000\t   0.00000\t   0.00000\t\n",
      "EP Qmax           \t    0.34204\t   0.01206\t   0.37976\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch    10\t Steps     111072\t Average Speed  2116.57\t Epsilon: 0.945637119999998\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    0.22800\t   0.51577\t   3.00000\t\n",
      "EP Loss           \t    0.00150\t   0.00075\t   0.00648\t\n",
      "EP Test Reward    \t    0.00000\t   0.00000\t   0.00000\t\n",
      "EP Qmax           \t    0.32025\t   0.03794\t   0.47323\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch    20\t Steps     210912\t Average Speed  2263.57\t Epsilon: 0.8962163199999962\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    0.27800\t   0.59052\t   4.00000\t\n",
      "EP Loss           \t    0.00079\t   0.00055\t   0.00538\t\n",
      "EP Test Reward    \t    0.00000\t   0.00000\t   0.00000\t\n",
      "EP Qmax           \t    0.30539\t   0.06197\t   0.52551\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch    30\t Steps     310752\t Average Speed  2328.55\t Epsilon: 0.8467955199999944\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    0.29300\t   0.61575\t   4.00000\t\n",
      "EP Loss           \t    0.00071\t   0.00053\t   0.00398\t\n",
      "EP Test Reward    \t    0.00000\t   0.00000\t   0.00000\t\n",
      "EP Qmax           \t    0.30145\t   0.06976\t   0.52697\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch    40\t Steps     410592\t Average Speed  2380.80\t Epsilon: 0.7973747199999925\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    0.32300\t   0.65013\t   4.00000\t\n",
      "EP Loss           \t    0.00066\t   0.00049\t   0.00460\t\n",
      "EP Test Reward    \t    0.00000\t   0.00000\t   0.00000\t\n",
      "EP Qmax           \t    0.31718\t   0.08579\t   0.58927\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch    50\t Steps     510432\t Average Speed  2409.40\t Epsilon: 0.7479539199999907\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    0.43200\t   0.71929\t   5.00000\t\n",
      "EP Loss           \t    0.00083\t   0.00056\t   0.00627\t\n",
      "EP Test Reward    \t    0.00000\t   0.00000\t   0.00000\t\n",
      "EP Qmax           \t    0.33713\t   0.09312\t   0.77651\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch    60\t Steps     610272\t Average Speed  2422.53\t Epsilon: 0.6985331199999889\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    0.61200\t   0.81207\t   5.00000\t\n",
      "EP Loss           \t    0.00136\t   0.00066\t   0.00523\t\n",
      "EP Test Reward    \t    0.00000\t   0.00000\t   0.00000\t\n",
      "EP Qmax           \t    0.41999\t   0.10976\t   0.70730\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch    70\t Steps     710112\t Average Speed  2438.10\t Epsilon: 0.6491123199999871\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    0.78600\t   0.95090\t   6.00000\t\n",
      "EP Loss           \t    0.00165\t   0.00076\t   0.00747\t\n",
      "EP Test Reward    \t    0.00000\t   0.00000\t   0.00000\t\n",
      "EP Qmax           \t    0.51206\t   0.12207\t   0.87281\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch    80\t Steps     811200\t Average Speed  2443.42\t Epsilon: 0.5990737599999852\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    0.90900\t   1.09669\t   6.00000\t\n",
      "EP Loss           \t    0.00228\t   0.00097\t   0.01002\t\n",
      "EP Test Reward    \t    0.00000\t   0.00000\t   0.00000\t\n",
      "EP Qmax           \t    0.63103\t   0.11114\t   0.94410\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch    90\t Steps     911040\t Average Speed  2450.29\t Epsilon: 0.5496529599999834\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    0.99400\t   1.27356\t   8.00000\t\n",
      "EP Loss           \t    0.00253\t   0.00105\t   0.01061\t\n",
      "EP Test Reward    \t    0.00000\t   0.00000\t   0.00000\t\n",
      "EP Qmax           \t    0.72007\t   0.10186\t   1.01878\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch   100\t Steps    1010880\t Average Speed  2441.43\t Epsilon: 0.5002321599999816\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    1.07500\t   1.49110\t  11.00000\t\n",
      "EP Loss           \t    0.00268\t   0.00186\t   0.02225\t\n",
      "EP Test Reward    \t    0.00000\t   0.00000\t   0.00000\t\n",
      "EP Qmax           \t    0.76852\t   0.10874\t   1.00449\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch   110\t Steps    1110720\t Average Speed  2438.72\t Epsilon: 0.4508113599999798\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    1.21400\t   1.52256\t  10.00000\t\n",
      "EP Loss           \t    0.00359\t   0.00211\t   0.01428\t\n",
      "EP Test Reward    \t    0.00000\t   0.00000\t   0.00000\t\n",
      "EP Qmax           \t    0.83741\t   0.10053\t   1.08256\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch   120\t Steps    1210560\t Average Speed  2424.84\t Epsilon: 0.40139055999997797\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    1.49100\t   1.79720\t  11.00000\t\n",
      "EP Loss           \t    0.00328\t   0.00138\t   0.00939\t\n",
      "EP Test Reward    \t    0.00000\t   0.00000\t   0.00000\t\n",
      "EP Qmax           \t    0.89977\t   0.08302\t   1.13721\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-29b4b3ec082a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_gpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-690fb77496b0>\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mLLs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-b32d600bd48e>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth_l1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msmooth_l1_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2479\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2481\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth_l1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2482\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    game = 'Breakout'\n",
    "    train(game)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plt.plot(TRs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# LLs = []\n",
    "# RRs = []\n",
    "# for epoch in range(10):\n",
    "#     model, model_target, Ls = train(model, model_target)\n",
    "#     # Rs = test(model)\n",
    "#     LLs += Ls\n",
    "#     # RRs += Rs\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plt.plot(Ls)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('replay.pkl', 'wb') as f:\n",
    "#     pickle.dump(replay, f, pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}