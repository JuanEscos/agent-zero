{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import cv2\n",
    "import copy\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import scipy\n",
    "import argparse\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import ray\n",
    "from scipy.signal import savgol_filter\n",
    "# plt.style.use('')\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "import torchvision as tv\n",
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "\n",
    "from src.common.atari_wrappers import wrap_deepmind, make_atari\n",
    "from src.common.utils import LinearSchedule, DataLoaderX, DataPrefetcher, ReplayDataset\n",
    "from src.common.vec_env import ShmemVecEnv, VecEnvWrapper, DummyVecEnv\n",
    "from src.agents.model import NatureCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "game = \"Breakout\"\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "num_env = 16\n",
    "num_actors = 8\n",
    "num_gpus = torch.cuda.device_count()\n",
    "num_cpus = mp.cpu_count()\n",
    "total_steps = int(1e7)\n",
    "epoches = 1000\n",
    "update_per_data = 8\n",
    "replay_size = int(1e6)\n",
    "discount = 0.99\n",
    "batch_size = 512\n",
    "lr = 1e-3\n",
    "\n",
    "target_net_update_freq = 250\n",
    "exploration_ratio = 0.2\n",
    "steps_per_epoch = total_steps // epoches \n",
    "replay = deque(maxlen=replay_size)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def make_env(game, episode_life=True, clip_rewards=True):\n",
    "    env = make_atari(f'{game}NoFrameskip-v4')\n",
    "    env = wrap_deepmind(env, episode_life=episode_life, clip_rewards=clip_rewards, frame_stack=True, scale=False, transpose_image=True)\n",
    "    return env\n",
    "\n",
    "test_env = make_env(game)\n",
    "state_shape = test_env.observation_space.shape\n",
    "action_dim = test_env.action_space.n\n",
    "epsilon_schedule = LinearSchedule(1.0, 0.01, int(total_steps * exploration_ratio))\n",
    "# \n",
    "# \n",
    "# model = NatureCNN(state_shape[0], action_dim).to(device)\n",
    "# model_target = copy.deepcopy(model)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('ckpt/e2.pth')['model'])\n",
    "# model_target = copy.deepcopy(model)\n",
    "# replay = deque(maxlen=100000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus=0.125)\n",
    "class Actor:\n",
    "    def __init__(self, rank):\n",
    "        self.envs = ShmemVecEnv([lambda: make_env(game) for _ in range(num_env)], context='fork')\n",
    "        self.R = np.zeros(num_env)\n",
    "        self.obs = self.envs.reset()\n",
    "        self.model = NatureCNN(state_shape[0], action_dim).cuda()\n",
    "        self.rank = rank\n",
    "    \n",
    "    def sample(self, epsilon, state_dict):\n",
    "        self.model.load_state_dict(state_dict)\n",
    "        steps = steps_per_epoch // (num_env * num_actors)\n",
    "        Rs, Qs = [], []\n",
    "        tic = time.time()\n",
    "        local_replay = deque(maxlen=replay_size)\n",
    "\n",
    "        for step in range(steps):\n",
    "            action_random = np.random.randint(0, action_dim, num_env)\n",
    "            st = torch.from_numpy(np.array(self.obs)).float().cuda() / 255.0\n",
    "            qs = self.model(st)\n",
    "            qs_max, qs_argmax = qs.max(dim=-1)\n",
    "            action_greedy = qs_argmax.tolist()\n",
    "            Qs.append(qs_max.mean().item())\n",
    "            action = [act_grd if p > epsilon else act_rnd for p, act_rnd, act_grd in zip(np.random.rand(num_env), action_random, action_greedy)]\n",
    "    \n",
    "            obs_next, reward, done, info = self.envs.step(action)\n",
    "            for entry in zip(self.obs, action, reward, obs_next, done):\n",
    "                local_replay.append(entry)\n",
    "            self.obs = obs_next\n",
    "            self.R += np.array(reward)\n",
    "            for idx, d in enumerate(done):\n",
    "                if d:\n",
    "                    Rs.append(self.R[idx])\n",
    "                    self.R[idx] = 0\n",
    "        toc = time.time()\n",
    "        # print(f\"Rank {self.rank}, Data Collection Time: {toc - tic}, Speed {steps_per_epoch / (toc - tic)}\")\n",
    "        return local_replay, Rs, Qs, self.rank\n",
    "\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.model = NatureCNN(state_shape[0], action_dim).cuda()\n",
    "        self.model_target = copy.deepcopy(self.model).cuda()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
    "        self.replay = deque(maxlen=replay_size)\n",
    "        self.update_steps = 0\n",
    "        \n",
    "    def get_datafetcher(self):\n",
    "        dataset = ReplayDataset(self.replay)\n",
    "        dataloader = DataLoaderX(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        datafetcher = DataPrefetcher(dataloader, device)\n",
    "        datafetcher.preload()\n",
    "        return datafetcher\n",
    "    \n",
    "    def append_data(self, data):\n",
    "        self.replay.extend(data)\n",
    "    \n",
    "    def train_step(self):\n",
    "        try:\n",
    "            data = self.prefetcher.next()\n",
    "        except:\n",
    "            self.prefetcher = self.get_datafetcher()\n",
    "            data = self.prefetcher.next()\n",
    "\n",
    "        states, actions, rewards, next_states, terminals = data\n",
    "        states = states.float() / 255.0\n",
    "        next_states = next_states.float() / 255.0\n",
    "        actions = actions.long()\n",
    "        terminals = terminals.float()\n",
    "        rewards = rewards.float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_next = self.model_target(next_states)\n",
    "            q_next_online = self.model(next_states)\n",
    "            q_next = q_next.gather(1, q_next_online.argmax(dim=-1).unsqueeze(-1)).squeeze(-1)\n",
    "            q_target = rewards + discount * (1 - terminals) * q_next\n",
    "\n",
    "        q = self.model(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "        loss = F.smooth_l1_loss(q, q_target)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.update_steps += 1\n",
    "\n",
    "        if self.update_steps % 500 == 0:\n",
    "            self.model_target.load_state_dict(self.model.state_dict())\n",
    "        return loss.detach()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def formated_print(var_name, xs):\n",
    "    print(\"{0}\\t {1:10.4f}\\t{2:10.4f}\\t{3:10.4f}\\t\".format(var_name, np.mean(xs), np.std(xs), np.max(xs)))\n",
    "\n",
    "def pipeline():\n",
    "    \n",
    "    actors = [Actor.remote(rank) for rank in range(num_actors)]\n",
    "    tester = Actor.remote(num_actors)\n",
    "    \n",
    "    agent = Agent()\n",
    "    sample_ops = [a.sample.remote(1.0, agent.model.state_dict()) for a in actors]\n",
    "    test_op = tester.sample.remote(0.01, agent.model.state_dict())\n",
    "    \n",
    "    TRRs, RRs, QQs, LLs = [], [], [], []\n",
    "    for local_replay, Rs, Qs, rank in ray.get(sample_ops + [test_op]):\n",
    "        if rank < num_actors:\n",
    "            agent.append_data(local_replay)\n",
    "            RRs += Rs\n",
    "            QQs += Qs\n",
    "        else:\n",
    "            TRRs += Rs\n",
    "            \n",
    "    \n",
    "    print(\"Warming up reward:\", np.mean(RRs), np.std(RRs), np.max(RRs))\n",
    "    print(\"Warming up Qmax:\", np.mean(QQs), np.std(QQs), np.max(QQs))        \n",
    "        \n",
    "    steps = 0\n",
    "    epoch = 0\n",
    "    tic = time.time()\n",
    "    while True:\n",
    "        done_id, sample_ops = ray.wait(sample_ops)\n",
    "        data = ray.get(done_id)\n",
    "        local_replay, Rs, Qs, rank = data[0]\n",
    "        \n",
    "        if rank < num_actors:\n",
    "            # Actor\n",
    "            agent.append_data(local_replay)\n",
    "            steps += len(local_replay)\n",
    "            epsilon = epsilon_schedule(len(local_replay))\n",
    "            sample_ops.append(actors[rank].sample.remote(epsilon, agent.model.state_dict()))\n",
    "            \n",
    "            RRs += Rs\n",
    "            QQs += Qs\n",
    "        else:\n",
    "            # Tester\n",
    "            sample_ops.append(tester.sample.remote(0.01, agent.model.state_dict()))\n",
    "            TRRs += Rs\n",
    "        \n",
    "        # Trainer\n",
    "        for _ in range(20):\n",
    "            loss = agent.train_step()\n",
    "            LLs.append(loss)\n",
    "        \n",
    "        if (steps // steps_per_epoch) > epoch:\n",
    "            if epoch % 10 == 0:\n",
    "                toc = time.time()\n",
    "                print(\"=\" * 100)\n",
    "                print(f\"Epoch {epoch:5d}t\\ Steps {steps:10d}t\\ Average Speed {steps / (toc - tic):8.2f}\\t Epsilon: {epsilon}\")\n",
    "                print('-' * 100)\n",
    "                formated_print(\"EP Training Reward\", RRs[-1000:])\n",
    "                formated_print(\"EP Loss           \", torch.stack(LLs).tolist()[-1000:])\n",
    "                formated_print(\"EP Test Reward    \", TRRs[-1000:])\n",
    "                formated_print(\"EP Qmax           \", QQs[-1000:])\n",
    "    \n",
    "                print(\"=\" * 100)\n",
    "                print(\" \" * 100)\n",
    "                \n",
    "                torch.save({\n",
    "                    'model': agent.model.state_dict(),\n",
    "                    'optim': agent.optimizer.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'epsilon': epsilon,\n",
    "                    'steps': steps,\n",
    "                    'Rs': RRs,\n",
    "                    'TRs': TRRs,\n",
    "                    'Qs': QQs,\n",
    "                    'Ls': LLs,\n",
    "                }, f'ckpt/{game}_e{epoch}')\n",
    "\n",
    "            epoch += 1\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "2020-08-03 05:24:22,913\tINFO resource_spec.py:212 -- Starting Ray with 231.01 GiB memory available for workers and up to 103.01 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-08-03 05:24:23,158\tWARNING services.py:923 -- Redis failed to start, retrying now.\n",
      "2020-08-03 05:24:23,500\tINFO services.py:1165 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n",
      "/home/bzhou/miniconda3/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "\u001b[2m\u001b[36m(pid=43496)\u001b[0m Logging to /tmp/openai-2020-08-03-05-24-25-472330\n",
      "\u001b[2m\u001b[36m(pid=43496)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=43494)\u001b[0m Logging to /tmp/openai-2020-08-03-05-24-25-499479\n",
      "\u001b[2m\u001b[36m(pid=43494)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=43487)\u001b[0m Logging to /tmp/openai-2020-08-03-05-24-25-468976\n",
      "\u001b[2m\u001b[36m(pid=43487)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=43476)\u001b[0m Logging to /tmp/openai-2020-08-03-05-24-25-481521\n",
      "\u001b[2m\u001b[36m(pid=43476)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=43497)\u001b[0m Logging to /tmp/openai-2020-08-03-05-24-25-482260\n",
      "\u001b[2m\u001b[36m(pid=43497)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=43491)\u001b[0m Logging to /tmp/openai-2020-08-03-05-24-25-475427\n",
      "\u001b[2m\u001b[36m(pid=43491)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=43454)\u001b[0m Logging to /tmp/openai-2020-08-03-05-24-25-541579\n",
      "\u001b[2m\u001b[36m(pid=43454)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=43490)\u001b[0m Logging to /tmp/openai-2020-08-03-05-24-25-532660\n",
      "\u001b[2m\u001b[36m(pid=43490)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=43477)\u001b[0m Logging to /tmp/openai-2020-08-03-05-24-25-536221\n",
      "\u001b[2m\u001b[36m(pid=43477)\u001b[0m Creating dummy env object to get spaces\n",
      "Warming up reward: 0.07333333333333333 0.2606828639468954 1.0\n",
      "Warming up Qmax: -0.43000209780457693 0.005602213005174384 -0.41125839948654175\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "ray.init(num_gpus=2)\n",
    "pipeline()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plt.plot(TRs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# LLs = []\n",
    "# RRs = []\n",
    "# for epoch in range(10):\n",
    "#     model, model_target, Ls = train(model, model_target)\n",
    "#     # Rs = test(model)\n",
    "#     LLs += Ls\n",
    "#     # RRs += Rs\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plt.plot(Ls)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('replay.pkl', 'wb') as f:\n",
    "#     pickle.dump(replay, f, pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}