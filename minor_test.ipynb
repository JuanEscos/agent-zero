{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import cv2\n",
    "import copy\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import scipy\n",
    "import argparse\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import ray\n",
    "from scipy.signal import savgol_filter\n",
    "# plt.style.use('')\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "import torchvision as tv\n",
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "\n",
    "from src.common.atari_wrappers import wrap_deepmind, make_atari\n",
    "from src.common.utils import LinearSchedule, DataLoaderX, DataPrefetcher, ReplayDataset\n",
    "from src.common.vec_env import ShmemVecEnv, VecEnvWrapper, DummyVecEnv\n",
    "from src.agents.model import NatureCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "num_env = 16\n",
    "num_actors = 8\n",
    "total_steps = int(1e7)\n",
    "epoches = 1000\n",
    "update_per_data = 8\n",
    "replay_size = int(1e6)\n",
    "discount = 0.99\n",
    "batch_size = 512\n",
    "lr = 1e-3\n",
    "\n",
    "target_net_update_freq = 250\n",
    "exploration_ratio = 0.2\n",
    "steps_per_epoch = total_steps // epoches \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def make_env(game, episode_life=True, clip_rewards=True):\n",
    "    env = make_atari(f'{game}NoFrameskip-v4')\n",
    "    env = wrap_deepmind(env, episode_life=episode_life, clip_rewards=clip_rewards, frame_stack=True, scale=False, transpose_image=True)\n",
    "    return env\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--game\", type=str, default=\"Breakout\")\n",
    "    parser.add_argument(\"--replay_size\", type=int, default=int(1e6))\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1024)\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-4)\n",
    "    parser.add_argument(\"--gpu_id\", type=int, default=9)\n",
    "    parser.add_argument(\"--discount\", type=float, default=0.99)\n",
    "    parser.add_argument(\"--exploration_steps\", type=int, default=20000)\n",
    "    parser.add_argument(\"--max_step\", type=int, default=int(1e7))\n",
    "    parser.add_argument(\"--seed\", type=int, default=1234)\n",
    "    parser.add_argument(\"--num_env\", type=int, default=16)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    print(\"input args:\\n\", json.dumps(vars(args), indent=4, separators=(\",\", \":\")))\n",
    "    return args\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus=0.125)\n",
    "class Actor:\n",
    "    def __init__(self, rank, game):\n",
    "        if rank < num_actors:\n",
    "            self.envs = ShmemVecEnv([lambda: make_env(game) for _ in range(num_env)], context='fork')\n",
    "        else:\n",
    "            self.envs = ShmemVecEnv([lambda: make_env(game, True, True) for _ in range(num_env)], context='fork')\n",
    "        self.R = np.zeros(num_env)\n",
    "        self.obs = self.envs.reset()\n",
    "        self.state_shape, self.action_dim = self.envs.observation_space.shape, self.envs.action_space.n\n",
    "        self.model = NatureCNN(self.state_shape[0], self.action_dim).cuda()\n",
    "        self.rank = rank\n",
    "    \n",
    "    def sample(self, epsilon, state_dict):\n",
    "        self.model.load_state_dict(state_dict)\n",
    "        steps = steps_per_epoch // (num_env * num_actors)\n",
    "        Rs, Qs = [], []\n",
    "        tic = time.time()\n",
    "        local_replay = deque(maxlen=replay_size)\n",
    "        for step in range(steps):\n",
    "            action_random = np.random.randint(0, self.action_dim, num_env)\n",
    "            st = torch.from_numpy(np.array(self.obs)).float().cuda() / 255.0\n",
    "            qs = self.model(st)\n",
    "            qs_max, qs_argmax = qs.max(dim=-1)\n",
    "            action_greedy = qs_argmax.tolist()\n",
    "            Qs.append(qs_max.mean().item())\n",
    "            action = [act_grd if p > epsilon else act_rnd for p, act_rnd, act_grd in zip(np.random.rand(num_env), action_random, action_greedy)]\n",
    "    \n",
    "            obs_next, reward, done, info = self.envs.step(action)\n",
    "            for entry in zip(self.obs, action, reward, obs_next, done):\n",
    "                local_replay.append(entry)\n",
    "            self.obs = obs_next\n",
    "            self.R += np.array(reward)\n",
    "            for idx, d in enumerate(done):\n",
    "                if d:\n",
    "                    Rs.append(self.R[idx])\n",
    "                    self.R[idx] = 0\n",
    "        toc = time.time()\n",
    "        # print(f\"Rank {self.rank}, Data Collection Time: {toc - tic}, Speed {steps_per_epoch / (toc - tic)}\")\n",
    "        return local_replay, Rs, Qs, self.rank\n",
    "\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, game):\n",
    "        test_env = make_env(game)\n",
    "        self.state_shape, self.action_dim = test_env.observation_space.shape, test_env.action_space.n\n",
    "        self.model = NatureCNN(self.state_shape[0], self.action_dim).cuda()\n",
    "        self.model_target = copy.deepcopy(self.model).cuda()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
    "        self.replay = deque(maxlen=replay_size)\n",
    "        self.update_steps = 0\n",
    "        self.device = torch.device('cuda:0')\n",
    "        \n",
    "    def get_datafetcher(self):\n",
    "        dataset = ReplayDataset(self.replay)\n",
    "        dataloader = DataLoaderX(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        datafetcher = DataPrefetcher(dataloader, self.device)\n",
    "        return datafetcher\n",
    "    \n",
    "    def append_data(self, data):\n",
    "        self.replay.extend(data)\n",
    "    \n",
    "    def train_step(self):\n",
    "        try:\n",
    "            data = self.prefetcher.next()\n",
    "        except:\n",
    "            self.prefetcher = self.get_datafetcher()\n",
    "            data = self.prefetcher.next()\n",
    "\n",
    "        states, actions, rewards, next_states, terminals = data\n",
    "        states = states.float() / 255.0\n",
    "        next_states = next_states.float() / 255.0\n",
    "        actions = actions.long()\n",
    "        terminals = terminals.float()\n",
    "        rewards = rewards.float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_next = self.model_target(next_states)\n",
    "            q_next_online = self.model(next_states)\n",
    "            q_next = q_next.gather(1, q_next_online.argmax(dim=-1).unsqueeze(-1)).squeeze(-1)\n",
    "            q_target = rewards + discount * (1 - terminals) * q_next\n",
    "\n",
    "        q = self.model(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "        loss = F.smooth_l1_loss(q, q_target)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.update_steps += 1\n",
    "\n",
    "        if self.update_steps % 250 == 0:\n",
    "            self.model_target.load_state_dict(self.model.state_dict())\n",
    "        return loss.detach()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def formated_print(var_name, xs):\n",
    "    print(\"{0}\\t {1:12.6f}\\t{2:12.6f}\\t{3:12.6f}\\t\".format(var_name, np.mean(xs), np.std(xs), np.max(xs)))\n",
    "\n",
    "def train(game):\n",
    "    ray.init()\n",
    "    \n",
    "    epsilon_schedule = LinearSchedule(1.0, 0.01, int(total_steps * exploration_ratio))\n",
    "    actors = [Actor.remote(rank, game) for rank in range(num_actors)]\n",
    "    tester = Actor.remote(num_actors, game)\n",
    "    \n",
    "    agent = Agent(game)\n",
    "    sample_ops = [a.sample.remote(1.0, agent.model.state_dict()) for a in actors]\n",
    "    test_op = tester.sample.remote(0.01, agent.model.state_dict())\n",
    "    \n",
    "    TRRs, RRs, QQs, LLs = [], [], [], []\n",
    "    for local_replay, Rs, Qs, rank in ray.get(sample_ops + [test_op]):\n",
    "        if rank < num_actors:\n",
    "            agent.append_data(local_replay)\n",
    "            RRs += Rs\n",
    "            QQs += Qs\n",
    "        else:\n",
    "            TRRs += Rs\n",
    "            \n",
    "    \n",
    "    print(\"Warming up reward:\", np.mean(RRs), np.std(RRs), np.max(RRs))\n",
    "    print(\"Warming up Qmax:\", np.mean(QQs), np.std(QQs), np.max(QQs))        \n",
    "        \n",
    "    steps = 0\n",
    "    epoch = 0\n",
    "    tic = time.time()\n",
    "    while True:\n",
    "        done_id, sample_ops = ray.wait(sample_ops)\n",
    "        data = ray.get(done_id)\n",
    "        local_replay, Rs, Qs, rank = data[0]\n",
    "        \n",
    "        if rank < num_actors:\n",
    "            # Actor\n",
    "            agent.append_data(local_replay)\n",
    "            steps += len(local_replay)\n",
    "            epsilon = epsilon_schedule(len(local_replay))\n",
    "            sample_ops.append(actors[rank].sample.remote(epsilon, agent.model.state_dict()))\n",
    "            RRs += Rs\n",
    "            QQs += Qs\n",
    "        else:\n",
    "            # Tester\n",
    "            sample_ops.append(tester.sample.remote(0.01, agent.model.state_dict()))\n",
    "            TRRs += Rs\n",
    "        \n",
    "        # Trainer\n",
    "        for _ in range(16):\n",
    "            loss = agent.train_step()\n",
    "            LLs.append(loss)\n",
    "        \n",
    "        if (steps // steps_per_epoch) > epoch:\n",
    "            if epoch % 10 == 0:\n",
    "                toc = time.time()\n",
    "                print(\"=\" * 100)\n",
    "                print(f\"Epoch: {epoch:5d}\\t Steps: {steps:10d}\\t Average Speed: {steps / (toc - tic):8.2f}\\t Epsilon: {epsilon}\")\n",
    "                print('-' * 100)\n",
    "                formated_print(\"EP Training Reward\", RRs[-1000:])\n",
    "                formated_print(\"EP Loss           \", torch.stack(LLs).tolist()[-1000:])\n",
    "                formated_print(\"EP Qmax           \", QQs[-1000:])\n",
    "                if len(TRRs) > 0:\n",
    "                    formated_print(\"EP Test Reward    \", TRRs[-1000:])\n",
    "                    \n",
    "                print(\"=\" * 100)\n",
    "                print(\" \" * 100)\n",
    "                \n",
    "                torch.save({\n",
    "                    'model': agent.model.state_dict(),\n",
    "                    'optim': agent.optimizer.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'epsilon': epsilon,\n",
    "                    'steps': steps,\n",
    "                    'Rs': RRs,\n",
    "                    'TRs': TRRs,\n",
    "                    'Qs': QQs,\n",
    "                    'Ls': LLs,\n",
    "                }, f'ckpt/{game}_e{epoch}')\n",
    "            \n",
    "            epoch += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "2020-08-03 07:09:15,532\tINFO resource_spec.py:212 -- Starting Ray with 231.15 GiB memory available for workers and up to 103.06 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-08-03 07:09:15,805\tWARNING services.py:923 -- Redis failed to start, retrying now.\n",
      "2020-08-03 07:09:16,177\tINFO services.py:1165 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n",
      "/home/bzhou/miniconda3/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "\u001b[2m\u001b[36m(pid=11028)\u001b[0m Logging to /tmp/openai-2020-08-03-07-09-18-002032\n",
      "\u001b[2m\u001b[36m(pid=11028)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=11030)\u001b[0m Logging to /tmp/openai-2020-08-03-07-09-17-981485\n",
      "\u001b[2m\u001b[36m(pid=11030)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=11057)\u001b[0m Logging to /tmp/openai-2020-08-03-07-09-18-002557\n",
      "\u001b[2m\u001b[36m(pid=11057)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=11072)\u001b[0m Logging to /tmp/openai-2020-08-03-07-09-17-993887\n",
      "\u001b[2m\u001b[36m(pid=11072)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=11076)\u001b[0m Logging to /tmp/openai-2020-08-03-07-09-17-982181\n",
      "\u001b[2m\u001b[36m(pid=11076)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=11086)\u001b[0m Logging to /tmp/openai-2020-08-03-07-09-17-996747\n",
      "\u001b[2m\u001b[36m(pid=11086)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=11067)\u001b[0m Logging to /tmp/openai-2020-08-03-07-09-17-984156\n",
      "\u001b[2m\u001b[36m(pid=11067)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=11037)\u001b[0m Logging to /tmp/openai-2020-08-03-07-09-18-001882\n",
      "\u001b[2m\u001b[36m(pid=11037)\u001b[0m Creating dummy env object to get spaces\n",
      "\u001b[2m\u001b[36m(pid=11063)\u001b[0m Logging to /tmp/openai-2020-08-03-07-09-18-009260\n",
      "\u001b[2m\u001b[36m(pid=11063)\u001b[0m Creating dummy env object to get spaces\n",
      "Warming up reward: 0.12186379928315412 0.3271284361033587 1.0\n",
      "Warming up Qmax: -0.13257039826697645 0.005888359608290227 -0.11684755980968475\n",
      "====================================================================================================\n",
      "Epoch:     0\t Steps:      11232\t Average Speed:  1574.28\t Epsilon: 0.9950579199999998\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     0.153846\t    0.396148\t    3.000000\t\n",
      "EP Loss           \t     0.032107\t    0.284610\t    3.387205\t\n",
      "EP Qmax           \t    -0.131465\t    0.008262\t   -0.113120\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    10\t Steps:     111072\t Average Speed:  2510.18\t Epsilon: 0.945637119999998\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     0.295000\t    0.629265\t    3.000000\t\n",
      "EP Loss           \t     0.001187\t    0.000742\t    0.005123\t\n",
      "EP Qmax           \t    -0.063723\t    0.041392\t    0.137294\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    20\t Steps:     210912\t Average Speed:  2676.70\t Epsilon: 0.8962163199999962\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     0.274000\t    0.604089\t    3.000000\t\n",
      "EP Loss           \t     0.000814\t    0.000545\t    0.004626\t\n",
      "EP Qmax           \t    -0.008980\t    0.057082\t    0.223163\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    30\t Steps:     310752\t Average Speed:  2769.34\t Epsilon: 0.8467955199999944\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     0.247000\t    0.598323\t    4.000000\t\n",
      "EP Loss           \t     0.000707\t    0.000440\t    0.003460\t\n",
      "EP Qmax           \t     0.044429\t    0.075585\t    0.330473\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    40\t Steps:     410592\t Average Speed:  2843.27\t Epsilon: 0.7973747199999925\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     0.250000\t    0.547266\t    3.000000\t\n",
      "EP Loss           \t     0.001010\t    0.000500\t    0.003674\t\n",
      "EP Qmax           \t     0.107755\t    0.081710\t    0.389278\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    50\t Steps:     510432\t Average Speed:  2906.31\t Epsilon: 0.7479539199999907\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     0.401000\t    0.701569\t    4.000000\t\n",
      "EP Loss           \t     0.005348\t    0.020951\t    0.468372\t\n",
      "EP Qmax           \t     0.257505\t    0.103758\t    0.633580\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    60\t Steps:     610272\t Average Speed:  2942.52\t Epsilon: 0.6985331199999889\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     0.654000\t    0.840407\t    5.000000\t\n",
      "EP Loss           \t     0.003553\t    0.001245\t    0.009954\t\n",
      "EP Qmax           \t     0.461798\t    0.098962\t    0.850547\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    70\t Steps:     710112\t Average Speed:  2963.57\t Epsilon: 0.6491123199999871\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     0.863000\t    1.024808\t    6.000000\t\n",
      "EP Loss           \t     0.003419\t    0.001081\t    0.008668\t\n",
      "EP Qmax           \t     0.581906\t    0.087162\t    0.922678\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    80\t Steps:     811200\t Average Speed:  2973.45\t Epsilon: 0.5990737599999852\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     0.891000\t    1.155473\t    6.000000\t\n",
      "EP Loss           \t     0.003405\t    0.000934\t    0.007710\t\n",
      "EP Qmax           \t     0.660185\t    0.086977\t    0.924383\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:    90\t Steps:     911040\t Average Speed:  2988.44\t Epsilon: 0.5496529599999834\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     1.013000\t    1.271547\t    6.000000\t\n",
      "EP Loss           \t     0.003225\t    0.000824\t    0.006642\t\n",
      "EP Qmax           \t     0.696112\t    0.084338\t    0.956191\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   100\t Steps:    1010880\t Average Speed:  2970.48\t Epsilon: 0.5002321599999816\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     1.125000\t    1.351064\t    7.000000\t\n",
      "EP Loss           \t     0.003666\t    0.003747\t    0.087314\t\n",
      "EP Qmax           \t     0.764269\t    0.084874\t    1.009705\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   110\t Steps:    1110720\t Average Speed:  2980.67\t Epsilon: 0.4508113599999798\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     1.416000\t    1.465928\t    7.000000\t\n",
      "EP Loss           \t     0.005918\t    0.003580\t    0.045241\t\n",
      "EP Qmax           \t     0.780517\t    0.074851\t    1.024295\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   120\t Steps:    1210560\t Average Speed:  2959.58\t Epsilon: 0.40139055999997797\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     1.707000\t    1.686165\t   13.000000\t\n",
      "EP Loss           \t     0.003630\t    0.001026\t    0.012669\t\n",
      "EP Qmax           \t     0.842070\t    0.088098\t    1.107147\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   130\t Steps:    1310400\t Average Speed:  2955.25\t Epsilon: 0.35196975999997615\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     1.806000\t    1.857516\t   12.000000\t\n",
      "EP Loss           \t     0.003869\t    0.001276\t    0.013612\t\n",
      "EP Qmax           \t     0.928112\t    0.092717\t    1.194583\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   140\t Steps:    1410240\t Average Speed:  2950.89\t Epsilon: 0.30254895999997433\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     2.248000\t    2.008605\t   13.000000\t\n",
      "EP Loss           \t     0.004062\t    0.001045\t    0.007771\t\n",
      "EP Qmax           \t     1.013223\t    0.108468\t    1.337426\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   150\t Steps:    1510080\t Average Speed:  2939.62\t Epsilon: 0.2531281599999725\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     2.647000\t    2.203268\t   15.000000\t\n",
      "EP Loss           \t     0.004332\t    0.001011\t    0.008516\t\n",
      "EP Qmax           \t     1.086403\t    0.092687\t    1.373501\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   160\t Steps:    1611168\t Average Speed:  2929.88\t Epsilon: 0.20308959999997278\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     3.147000\t    2.424746\t   14.000000\t\n",
      "EP Loss           \t     0.004637\t    0.001128\t    0.009723\t\n",
      "EP Qmax           \t     1.191764\t    0.099515\t    1.499921\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   170\t Steps:    1711008\t Average Speed:  2926.65\t Epsilon: 0.15366879999997318\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     3.931000\t    3.018317\t   19.000000\t\n",
      "EP Loss           \t     0.005307\t    0.001351\t    0.011014\t\n",
      "EP Qmax           \t     1.274618\t    0.096699\t    1.608484\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   180\t Steps:    1810848\t Average Speed:  2921.08\t Epsilon: 0.10424799999997358\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     4.731000\t    3.552835\t   22.000000\t\n",
      "EP Loss           \t     0.004845\t    0.001232\t    0.011265\t\n",
      "EP Qmax           \t     1.324283\t    0.083071\t    1.540149\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   190\t Steps:    1910688\t Average Speed:  2907.48\t Epsilon: 0.054827199999973895\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     5.444000\t    4.073679\t   23.000000\t\n",
      "EP Loss           \t     0.005017\t    0.001364\t    0.010175\t\n",
      "EP Qmax           \t     1.380856\t    0.089357\t    1.692336\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   200\t Steps:    2010528\t Average Speed:  2899.23\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     6.402000\t    4.812733\t   40.000000\t\n",
      "EP Loss           \t     0.005661\t    0.001567\t    0.011781\t\n",
      "EP Qmax           \t     1.421352\t    0.087732\t    1.685989\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   210\t Steps:    2110368\t Average Speed:  2885.76\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     7.307000\t    5.933865\t   40.000000\t\n",
      "EP Loss           \t     0.005572\t    0.001428\t    0.011691\t\n",
      "EP Qmax           \t     1.487956\t    0.090156\t    1.727210\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   220\t Steps:    2210208\t Average Speed:  2869.13\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     8.284000\t    6.991662\t   41.000000\t\n",
      "EP Loss           \t     0.006045\t    0.001638\t    0.012281\t\n",
      "EP Qmax           \t     1.510919\t    0.100846\t    1.837096\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   230\t Steps:    2310048\t Average Speed:  2842.22\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     8.599000\t    7.655599\t   41.000000\t\n",
      "EP Loss           \t     0.006639\t    0.001749\t    0.012989\t\n",
      "EP Qmax           \t     1.573285\t    0.095962\t    1.876725\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   240\t Steps:    2411136\t Average Speed:  2826.46\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     9.265000\t    8.431179\t   48.000000\t\n",
      "EP Loss           \t     0.006673\t    0.001769\t    0.013201\t\n",
      "EP Qmax           \t     1.632057\t    0.096256\t    1.922424\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   250\t Steps:    2510976\t Average Speed:  2817.74\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t     9.635000\t    8.855155\t   58.000000\t\n",
      "EP Loss           \t     0.007455\t    0.002018\t    0.015778\t\n",
      "EP Qmax           \t     1.746995\t    0.094076\t    1.997505\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Epoch:   260\t Steps:    2610816\t Average Speed:  2819.94\t Epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EP Training Reward\t    10.250000\t    9.428971\t   74.000000\t\n",
      "EP Loss           \t     0.007228\t    0.002112\t    0.018006\t\n",
      "EP Qmax           \t     1.760590\t    0.092818\t    2.007807\t\n",
      "EP Test Reward    \t     0.000000\t    0.000000\t    0.000000\t\n",
      "====================================================================================================\n",
      "                                                                                                    \n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # args = parse_arguments()\n",
    "    # game = args.game\n",
    "    game = \"Breakout\"\n",
    "    train(game)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plt.plot(TRs)\n",
    "# device = torch.device('cuda:0')\n",
    "# replay = deque(maxlen=10000)\n",
    "# datas = [np.random.randn(1000, 32)] * 5\n",
    "# for entry in zip(*datas):\n",
    "#     replay.append(entry)\n",
    "#     \n",
    "# dataset = ReplayDataset(replay)\n",
    "# dataloader = DataLoaderX(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "# print(next(iter(dataloader)))\n",
    "# datafetcher = DataPrefetcher(dataloader, device)\n",
    "# datafetcher.preload()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# LLs = []\n",
    "# RRs = []\n",
    "# for epoch in range(10):\n",
    "#     model, model_target, Ls = train(model, model_target)\n",
    "#     # Rs = test(model)\n",
    "#     LLs += Ls\n",
    "#     # RRs += Rs\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plt.plot(Ls)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('replay.pkl', 'wb') as f:\n",
    "#     pickle.dump(replay, f, pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}